{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2e68a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_21444\\1021965397.py:2: DtypeWarning: Columns (1,6,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_jobs = pd.read_csv(\"all_jobs.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "all_jobs = pd.read_csv(\"all_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e974a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_jobs.copy()\n",
    "df.shape, df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312e6c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "responsibilities    0.445337\n",
       "qualifications      0.445337\n",
       "country             0.445337\n",
       "skills_raw          0.001042\n",
       "company             0.000037\n",
       "job_key             0.000034\n",
       "location            0.000007\n",
       "source              0.000000\n",
       "job_title           0.000000\n",
       "job_description     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdcc28d",
   "metadata": {},
   "source": [
    "1-clean all jobs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079bda53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "âœ… Finished: full cleaned dataset written to: all_jobs_clean_full.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "input_path  = \"all_jobs.csv\"               # merged big file (6GB)\n",
    "output_path = \"all_jobs_clean_full.csv\"    # final cleaned file\n",
    "\n",
    "chunksize = 50_000\n",
    "\n",
    "# ðŸ” for GLOBAL duplicate removal\n",
    "seen_job_keys = set()       # for drop_duplicates(subset=[\"job_key\"])\n",
    "seen_composite_keys = set() # for drop_duplicates subset combo\n",
    "\n",
    "\n",
    "# 4ï¸âƒ£ your cleaning function, vectorized-style\n",
    "def clean_text_series(s: pd.Series):\n",
    "    return (\n",
    "        s.astype(str)\n",
    "         .str.replace(\"\\n\", \" \", regex=False)\n",
    "         .str.replace(\"\\r\", \" \", regex=False)\n",
    "         .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "         .str.strip()\n",
    "    )\n",
    "\n",
    "# 3ï¸âƒ£ same logic: empty row = no title, no desc, no skills\n",
    "def drop_fully_empty_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def is_empty(x):\n",
    "        return (pd.isna(x)) or (str(x).strip() == \"\")\n",
    "\n",
    "    mask_keep = ~(\n",
    "        df[\"job_title\"].apply(is_empty) &\n",
    "        df[\"job_description\"].apply(is_empty) &\n",
    "        df[\"skills_raw\"].apply(is_empty)\n",
    "    )\n",
    "    return df[mask_keep].copy()\n",
    "\n",
    "\n",
    "def make_composite_key(row):\n",
    "    # like your subset=[\"job_title\",\"company\",\"location\",\"job_description\"]\n",
    "    parts = [\n",
    "        str(row.get(\"job_title\", \"\")).lower(),\n",
    "        str(row.get(\"company\", \"\")).lower(),\n",
    "        str(row.get(\"location\", \"\")).lower(),\n",
    "        str(row.get(\"job_description\", \"\")).lower()[:200],  # truncated desc to reduce memory\n",
    "    ]\n",
    "    return \"||\".join(parts)\n",
    "\n",
    "\n",
    "def process_chunk(chunk: pd.DataFrame, first: bool):\n",
    "    global seen_job_keys, seen_composite_keys\n",
    "\n",
    "    # drop useless very sparse columns if they still exist\n",
    "    for col in [\"responsibilities\", \"qualifications\",\"country\"]:\n",
    "        if col in chunk.columns:\n",
    "            chunk = chunk.drop(columns=[col])\n",
    "\n",
    "    # basic missing handling for text columns\n",
    "    for col in [\"job_title\", \"job_description\", \"skills_raw\"]:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = chunk[col].fillna(\"\")\n",
    "\n",
    "    # ðŸ‘‰ step 3: drop rows with empty title+desc+skills\n",
    "    chunk = drop_fully_empty_rows(chunk)\n",
    "\n",
    "    # ðŸ‘‰ step 4: clean text (job_title, job_description, skills_raw, location, company, country)\n",
    "    cols_to_clean = [\"job_title\", \"job_description\", \"skills_raw\",\n",
    "                     \"location\", \"company\", \"country\"]\n",
    "    for col in cols_to_clean:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = clean_text_series(chunk[col])\n",
    "\n",
    "    # ðŸ‘‰ step 5: filter short title / desc\n",
    "    if \"job_title\" in chunk.columns:\n",
    "        chunk = chunk[chunk[\"job_title\"].str.len() >= 5]\n",
    "    if \"job_description\" in chunk.columns:\n",
    "        chunk = chunk[chunk[\"job_description\"].str.len() >= 30]\n",
    "\n",
    "    # ðŸ‘‰ step 6.1: global dedup by job_key (if exists)\n",
    "    if \"job_key\" in chunk.columns:\n",
    "        mask_new_key = ~chunk[\"job_key\"].isin(seen_job_keys)\n",
    "        chunk = chunk[mask_new_key].copy()\n",
    "        seen_job_keys.update(chunk[\"job_key\"].tolist())\n",
    "\n",
    "    # ðŸ‘‰ step 6.2: global dedup by combo (title+company+location+desc)\n",
    "    #     (does what your drop_duplicates(subset=[...]) did, but globally)\n",
    "    chunk[\"__dedup_key__\"] = chunk.apply(make_composite_key, axis=1)\n",
    "    mask_new_combo = ~chunk[\"__dedup_key__\"].isin(seen_composite_keys)\n",
    "    chunk = chunk[mask_new_combo].copy()\n",
    "    seen_composite_keys.update(chunk[\"__dedup_key__\"].tolist())\n",
    "    chunk = chunk.drop(columns=[\"__dedup_key__\"])\n",
    "\n",
    "    return chunk\n",
    "\n",
    "\n",
    "def run_full_clean():\n",
    "    first = True\n",
    "\n",
    "    for chunk in pd.read_csv(input_path, chunksize=chunksize):\n",
    "        print(\"Processing new chunk...\")\n",
    "        cleaned_chunk = process_chunk(chunk, first)\n",
    "\n",
    "        cleaned_chunk.to_csv(\n",
    "            output_path,\n",
    "            mode=\"w\" if first else \"a\",\n",
    "            header=first,\n",
    "            index=False\n",
    "        )\n",
    "        first = False\n",
    "\n",
    "    print(\"âœ… Finished: full cleaned dataset written to:\", output_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_clean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb652b",
   "metadata": {},
   "source": [
    "2- clean skill migration public dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e7cb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17617, 12)\n",
      "  country_code country_name   wb_income   wb_region  skill_group_id  \\\n",
      "0           af  Afghanistan  Low income  South Asia            2549   \n",
      "1           af  Afghanistan  Low income  South Asia            2608   \n",
      "2           af  Afghanistan  Low income  South Asia            3806   \n",
      "3           af  Afghanistan  Low income  South Asia           50321   \n",
      "4           af  Afghanistan  Low income  South Asia            1606   \n",
      "\n",
      "          skill_group_category        skill_group_name  net_per_10K_2015  \\\n",
      "0                  Tech Skills  Information Management           -791.59   \n",
      "1              Business Skills  Operational Efficiency          -1610.25   \n",
      "2  Specialized Industry Skills       National Security          -1731.45   \n",
      "3                  Tech Skills        Software Testing           -957.50   \n",
      "4  Specialized Industry Skills                    Navy          -1510.71   \n",
      "\n",
      "   net_per_10K_2016  net_per_10K_2017  net_per_10K_2018  net_per_10K_2019  \n",
      "0           -705.88           -550.04           -680.92          -1208.79  \n",
      "1           -933.55           -776.06           -532.22           -790.09  \n",
      "2           -769.68           -756.59           -600.44           -767.64  \n",
      "3           -828.54           -964.73           -406.50           -739.51  \n",
      "4           -841.17           -842.32           -581.71           -718.64  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load raw file\n",
    "skills_raw = pd.read_csv(\"skill_migration_public.csv\")\n",
    "print(skills_raw.shape)\n",
    "print(skills_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2a28bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249, 2)\n",
      "         skill_group_name         skill_group_category\n",
      "0  information management                  tech skills\n",
      "1  operational efficiency              business skills\n",
      "2       national security  specialized industry skills\n",
      "3        software testing                  tech skills\n",
      "4                    navy  specialized industry skills\n"
     ]
    }
   ],
   "source": [
    "# 2. Keep only the useful columns\n",
    "skills = skills_raw[[\n",
    "    \"skill_group_name\",\n",
    "    \"skill_group_category\"\n",
    "]].copy()\n",
    "\n",
    "# 3. Drop rows where skill name is missing\n",
    "skills = skills[skills[\"skill_group_name\"].notna()].copy()\n",
    "\n",
    "# 4. Normalize text (lowercase, strip)\n",
    "for col in [\"skill_group_name\", \"skill_group_category\"]:\n",
    "    if col in skills.columns:\n",
    "        skills[col] = (\n",
    "            skills[col]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.lower()\n",
    "        )\n",
    "\n",
    "# 5. Drop exact duplicates on (skill_group_name, skill_group_category)\n",
    "skills = skills.drop_duplicates(subset=[\"skill_group_name\", \"skill_group_category\"])\n",
    "\n",
    "# 6. If one skill appears with multiple categories, we keep the first one\n",
    "skills = skills.drop_duplicates(subset=[\"skill_group_name\"], keep=\"first\")\n",
    "\n",
    "print(skills.shape)\n",
    "print(skills.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90c9a3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved cleaned skills dictionary to skill_migration_clean.csv\n"
     ]
    }
   ],
   "source": [
    "skills.to_csv(\"skill_migration_clean.csv\", index=False)\n",
    "print(\"âœ… Saved cleaned skills dictionary to skill_migration_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187973c",
   "metadata": {},
   "source": [
    "3- Map skills onto all_jobs_clean_full.csv with chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d75ba9",
   "metadata": {},
   "source": [
    "Cell 1: Load skills dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816af3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 249 skills.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "skills_meta = pd.read_csv(\"skill_migration_clean.csv\")\n",
    "\n",
    "skill_dict = (\n",
    "    skills_meta[[\"skill_group_name\", \"skill_group_category\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"skill_group_name\")[\"skill_group_category\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(skill_dict)} skills.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d284e",
   "metadata": {},
   "source": [
    "Cell 2: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb31b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_skills(raw):\n",
    "    if pd.isna(raw):\n",
    "        return []\n",
    "    text = str(raw).strip().lower()\n",
    "    if not text:\n",
    "        return []\n",
    "    parts = re.split(r\",|/|;|\\||\\+\", text)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def map_skill_categories(skill_list):\n",
    "    cats = set()\n",
    "    for s in skill_list:\n",
    "        cat = skill_dict.get(s)\n",
    "        if cat is None:\n",
    "            cat = \"other\"\n",
    "        cats.add(cat)\n",
    "    if not cats:\n",
    "        return \"\"\n",
    "    return \",\".join(sorted(cats))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecced3b",
   "metadata": {},
   "source": [
    "Cell 3: Process all_jobs_clean_full.csv with chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b016c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 34556 rows...\n",
      "âœ… Done ! Final dataset: all_jobs_mapped.csv\n"
     ]
    }
   ],
   "source": [
    "input_path  = \"all_jobs_clean_full.csv\"\n",
    "output_path = \"all_jobs_mapped.csv\"\n",
    "chunksize   = 50_000\n",
    "\n",
    "first = True\n",
    "\n",
    "for chunk in pd.read_csv(input_path, chunksize=chunksize):\n",
    "    print(f\"Processing {len(chunk)} rows...\")\n",
    "\n",
    "    if \"skills_raw\" not in chunk.columns:\n",
    "        chunk[\"skills_raw\"] = \"\"\n",
    "\n",
    "    chunk[\"skill_list\"] = chunk[\"skills_raw\"].apply(parse_skills)\n",
    "    chunk[\"skill_categories\"] = chunk[\"skill_list\"].apply(map_skill_categories)\n",
    "\n",
    "    chunk.to_csv(\n",
    "        output_path,\n",
    "        mode=\"w\" if first else \"a\",\n",
    "        header=first,\n",
    "        index=False\n",
    "    )\n",
    "    first = False\n",
    "\n",
    "print(\"âœ… Done ! Final dataset:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ed1ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6904.832558631897 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.getsize(\"all_jobs_mapped.csv\") / (1024*1024), \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "047c9a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_29532\\1045503748.py:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"all_jobs_mapped.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2884556, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"all_jobs_mapped.csv\")\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d75f12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source', 'job_key', 'job_title', 'job_description', 'skills_raw',\n",
       "       'location', 'company', 'skill_list', 'skill_categories'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac8c367",
   "metadata": {},
   "source": [
    "TRAINING STEPS TO FIND MODELS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed89d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"all_jobs_mapped.csv\"\n",
    "\n",
    "# Load a manageable sample for experiments (you can increase nrows later)\n",
    "df = pd.read_csv(path, nrows=500_000)\n",
    "\n",
    "df.shape, df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d51bc",
   "metadata": {},
   "source": [
    "--Ensure skill_list is really a lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a253efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def to_skill_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(x)\n",
    "        if isinstance(val, list):\n",
    "            return [str(s).strip().lower() for s in val]\n",
    "    except:\n",
    "        pass\n",
    "    return [s.strip().lower() for s in str(x).split(\",\") if s.strip()]\n",
    "\n",
    "df[\"skill_list\"] = df[\"skill_list\"].apply(to_skill_list)\n",
    "df[\"skill_categories\"] = df[\"skill_categories\"].fillna(\"\").astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191351d1",
   "metadata": {},
   "source": [
    "Association Rules: 3 Models "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
