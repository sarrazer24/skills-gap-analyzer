{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2e68a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_21444\\1021965397.py:2: DtypeWarning: Columns (1,6,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_jobs = pd.read_csv(\"all_jobs.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "all_jobs = pd.read_csv(\"all_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e974a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_jobs.copy()\n",
    "df.shape, df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312e6c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "responsibilities    0.445337\n",
       "qualifications      0.445337\n",
       "country             0.445337\n",
       "skills_raw          0.001042\n",
       "company             0.000037\n",
       "job_key             0.000034\n",
       "location            0.000007\n",
       "source              0.000000\n",
       "job_title           0.000000\n",
       "job_description     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdcc28d",
   "metadata": {},
   "source": [
    "1-clean all jobs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079bda53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "Processing new chunk...\n",
      "âœ… Finished: full cleaned dataset written to: all_jobs_clean_full.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "input_path  = \"all_jobs.csv\"               # merged big file (6GB)\n",
    "output_path = \"all_jobs_clean_full.csv\"    # final cleaned file\n",
    "\n",
    "chunksize = 50_000\n",
    "\n",
    "# ğŸ” for GLOBAL duplicate removal\n",
    "seen_job_keys = set()       # for drop_duplicates(subset=[\"job_key\"])\n",
    "seen_composite_keys = set() # for drop_duplicates subset combo\n",
    "\n",
    "\n",
    "# 4ï¸âƒ£ your cleaning function, vectorized-style\n",
    "def clean_text_series(s: pd.Series):\n",
    "    return (\n",
    "        s.astype(str)\n",
    "         .str.replace(\"\\n\", \" \", regex=False)\n",
    "         .str.replace(\"\\r\", \" \", regex=False)\n",
    "         .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "         .str.strip()\n",
    "    )\n",
    "\n",
    "# 3ï¸âƒ£ same logic: empty row = no title, no desc, no skills\n",
    "def drop_fully_empty_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def is_empty(x):\n",
    "        return (pd.isna(x)) or (str(x).strip() == \"\")\n",
    "\n",
    "    mask_keep = ~(\n",
    "        df[\"job_title\"].apply(is_empty) &\n",
    "        df[\"job_description\"].apply(is_empty) &\n",
    "        df[\"skills_raw\"].apply(is_empty)\n",
    "    )\n",
    "    return df[mask_keep].copy()\n",
    "\n",
    "\n",
    "def make_composite_key(row):\n",
    "    # like your subset=[\"job_title\",\"company\",\"location\",\"job_description\"]\n",
    "    parts = [\n",
    "        str(row.get(\"job_title\", \"\")).lower(),\n",
    "        str(row.get(\"company\", \"\")).lower(),\n",
    "        str(row.get(\"location\", \"\")).lower(),\n",
    "        str(row.get(\"job_description\", \"\")).lower()[:200],  # truncated desc to reduce memory\n",
    "    ]\n",
    "    return \"||\".join(parts)\n",
    "\n",
    "\n",
    "def process_chunk(chunk: pd.DataFrame, first: bool):\n",
    "    global seen_job_keys, seen_composite_keys\n",
    "\n",
    "    # drop useless very sparse columns if they still exist\n",
    "    for col in [\"responsibilities\", \"qualifications\",\"country\"]:\n",
    "        if col in chunk.columns:\n",
    "            chunk = chunk.drop(columns=[col])\n",
    "\n",
    "    # basic missing handling for text columns\n",
    "    for col in [\"job_title\", \"job_description\", \"skills_raw\"]:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = chunk[col].fillna(\"\")\n",
    "\n",
    "    # ğŸ‘‰ step 3: drop rows with empty title+desc+skills\n",
    "    chunk = drop_fully_empty_rows(chunk)\n",
    "\n",
    "    # ğŸ‘‰ step 4: clean text (job_title, job_description, skills_raw, location, company, country)\n",
    "    cols_to_clean = [\"job_title\", \"job_description\", \"skills_raw\",\n",
    "                     \"location\", \"company\", \"country\"]\n",
    "    for col in cols_to_clean:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = clean_text_series(chunk[col])\n",
    "\n",
    "    # ğŸ‘‰ step 5: filter short title / desc\n",
    "    if \"job_title\" in chunk.columns:\n",
    "        chunk = chunk[chunk[\"job_title\"].str.len() >= 5]\n",
    "    if \"job_description\" in chunk.columns:\n",
    "        chunk = chunk[chunk[\"job_description\"].str.len() >= 30]\n",
    "\n",
    "    # ğŸ‘‰ step 6.1: global dedup by job_key (if exists)\n",
    "    if \"job_key\" in chunk.columns:\n",
    "        mask_new_key = ~chunk[\"job_key\"].isin(seen_job_keys)\n",
    "        chunk = chunk[mask_new_key].copy()\n",
    "        seen_job_keys.update(chunk[\"job_key\"].tolist())\n",
    "\n",
    "    # ğŸ‘‰ step 6.2: global dedup by combo (title+company+location+desc)\n",
    "    #     (does what your drop_duplicates(subset=[...]) did, but globally)\n",
    "    chunk[\"__dedup_key__\"] = chunk.apply(make_composite_key, axis=1)\n",
    "    mask_new_combo = ~chunk[\"__dedup_key__\"].isin(seen_composite_keys)\n",
    "    chunk = chunk[mask_new_combo].copy()\n",
    "    seen_composite_keys.update(chunk[\"__dedup_key__\"].tolist())\n",
    "    chunk = chunk.drop(columns=[\"__dedup_key__\"])\n",
    "\n",
    "    return chunk\n",
    "\n",
    "\n",
    "def run_full_clean():\n",
    "    first = True\n",
    "\n",
    "    for chunk in pd.read_csv(input_path, chunksize=chunksize):\n",
    "        print(\"Processing new chunk...\")\n",
    "        cleaned_chunk = process_chunk(chunk, first)\n",
    "\n",
    "        cleaned_chunk.to_csv(\n",
    "            output_path,\n",
    "            mode=\"w\" if first else \"a\",\n",
    "            header=first,\n",
    "            index=False\n",
    "        )\n",
    "        first = False\n",
    "\n",
    "    print(\"âœ… Finished: full cleaned dataset written to:\", output_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_clean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb652b",
   "metadata": {},
   "source": [
    "2- clean skill migration public dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e7cb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17617, 12)\n",
      "  country_code country_name   wb_income   wb_region  skill_group_id  \\\n",
      "0           af  Afghanistan  Low income  South Asia            2549   \n",
      "1           af  Afghanistan  Low income  South Asia            2608   \n",
      "2           af  Afghanistan  Low income  South Asia            3806   \n",
      "3           af  Afghanistan  Low income  South Asia           50321   \n",
      "4           af  Afghanistan  Low income  South Asia            1606   \n",
      "\n",
      "          skill_group_category        skill_group_name  net_per_10K_2015  \\\n",
      "0                  Tech Skills  Information Management           -791.59   \n",
      "1              Business Skills  Operational Efficiency          -1610.25   \n",
      "2  Specialized Industry Skills       National Security          -1731.45   \n",
      "3                  Tech Skills        Software Testing           -957.50   \n",
      "4  Specialized Industry Skills                    Navy          -1510.71   \n",
      "\n",
      "   net_per_10K_2016  net_per_10K_2017  net_per_10K_2018  net_per_10K_2019  \n",
      "0           -705.88           -550.04           -680.92          -1208.79  \n",
      "1           -933.55           -776.06           -532.22           -790.09  \n",
      "2           -769.68           -756.59           -600.44           -767.64  \n",
      "3           -828.54           -964.73           -406.50           -739.51  \n",
      "4           -841.17           -842.32           -581.71           -718.64  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load raw file\n",
    "skills_raw = pd.read_csv(\"skill_migration_public.csv\")\n",
    "print(skills_raw.shape)\n",
    "print(skills_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2a28bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249, 2)\n",
      "         skill_group_name         skill_group_category\n",
      "0  information management                  tech skills\n",
      "1  operational efficiency              business skills\n",
      "2       national security  specialized industry skills\n",
      "3        software testing                  tech skills\n",
      "4                    navy  specialized industry skills\n"
     ]
    }
   ],
   "source": [
    "# 2. Keep only the useful columns\n",
    "skills = skills_raw[[\n",
    "    \"skill_group_name\",\n",
    "    \"skill_group_category\"\n",
    "]].copy()\n",
    "\n",
    "# 3. Drop rows where skill name is missing\n",
    "skills = skills[skills[\"skill_group_name\"].notna()].copy()\n",
    "\n",
    "# 4. Normalize text (lowercase, strip)\n",
    "for col in [\"skill_group_name\", \"skill_group_category\"]:\n",
    "    if col in skills.columns:\n",
    "        skills[col] = (\n",
    "            skills[col]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.lower()\n",
    "        )\n",
    "\n",
    "# 5. Drop exact duplicates on (skill_group_name, skill_group_category)\n",
    "skills = skills.drop_duplicates(subset=[\"skill_group_name\", \"skill_group_category\"])\n",
    "\n",
    "# 6. If one skill appears with multiple categories, we keep the first one\n",
    "skills = skills.drop_duplicates(subset=[\"skill_group_name\"], keep=\"first\")\n",
    "\n",
    "print(skills.shape)\n",
    "print(skills.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90c9a3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved cleaned skills dictionary to skill_migration_clean.csv\n"
     ]
    }
   ],
   "source": [
    "skills.to_csv(\"skill_migration_clean.csv\", index=False)\n",
    "print(\"âœ… Saved cleaned skills dictionary to skill_migration_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187973c",
   "metadata": {},
   "source": [
    "3- Map skills onto all_jobs_clean_full.csv with chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d75ba9",
   "metadata": {},
   "source": [
    "Cell 1: Load skills dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816af3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 249 skills.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "skills_meta = pd.read_csv(\"skill_migration_clean.csv\")\n",
    "\n",
    "skill_dict = (\n",
    "    skills_meta[[\"skill_group_name\", \"skill_group_category\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"skill_group_name\")[\"skill_group_category\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(skill_dict)} skills.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d284e",
   "metadata": {},
   "source": [
    "Cell 2: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb31b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_skills(raw):\n",
    "    if pd.isna(raw):\n",
    "        return []\n",
    "    text = str(raw).strip().lower()\n",
    "    if not text:\n",
    "        return []\n",
    "    parts = re.split(r\",|/|;|\\||\\+\", text)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def map_skill_categories(skill_list):\n",
    "    cats = set()\n",
    "    for s in skill_list:\n",
    "        cat = skill_dict.get(s)\n",
    "        if cat is None:\n",
    "            cat = \"other\"\n",
    "        cats.add(cat)\n",
    "    if not cats:\n",
    "        return \"\"\n",
    "    return \",\".join(sorted(cats))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecced3b",
   "metadata": {},
   "source": [
    "Cell 3: Process all_jobs_clean_full.csv with chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b016c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 50000 rows...\n",
      "Processing 34556 rows...\n",
      "âœ… Done ! Final dataset: all_jobs_mapped.csv\n"
     ]
    }
   ],
   "source": [
    "input_path  = \"all_jobs_clean_full.csv\"\n",
    "output_path = \"all_jobs_mapped.csv\"\n",
    "chunksize   = 50_000\n",
    "\n",
    "first = True\n",
    "\n",
    "for chunk in pd.read_csv(input_path, chunksize=chunksize):\n",
    "    print(f\"Processing {len(chunk)} rows...\")\n",
    "\n",
    "    if \"skills_raw\" not in chunk.columns:\n",
    "        chunk[\"skills_raw\"] = \"\"\n",
    "\n",
    "    chunk[\"skill_list\"] = chunk[\"skills_raw\"].apply(parse_skills)\n",
    "    chunk[\"skill_categories\"] = chunk[\"skill_list\"].apply(map_skill_categories)\n",
    "\n",
    "    chunk.to_csv(\n",
    "        output_path,\n",
    "        mode=\"w\" if first else \"a\",\n",
    "        header=first,\n",
    "        index=False\n",
    "    )\n",
    "    first = False\n",
    "\n",
    "print(\"âœ… Done ! Final dataset:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ed1ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6904.832558631897 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.getsize(\"all_jobs_mapped.csv\") / (1024*1024), \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047c9a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_23180\\1045503748.py:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"all_jobs_mapped.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2884556, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"all_jobs_mapped.csv\")\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d75f12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source', 'job_key', 'job_title', 'job_description', 'skills_raw',\n",
       "       'location', 'company', 'skill_list', 'skill_categories'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac8c367",
   "metadata": {},
   "source": [
    "TRAINING STEPS TO FIND MODELS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed89d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_4476\\3013172477.py:6: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2884556, 9),\n",
       " Index(['source', 'job_key', 'job_title', 'job_description', 'skills_raw',\n",
       "        'location', 'company', 'skill_list', 'skill_categories'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"all_jobs_mapped.csv\"\n",
    "\n",
    "# Load a manageable sample for experiments (you can increase nrows later)\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df.shape, df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d51bc",
   "metadata": {},
   "source": [
    "--Ensure skill_list is really a lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a253efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def to_skill_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(x)\n",
    "        if isinstance(val, list):\n",
    "            return [str(s).strip().lower() for s in val]\n",
    "    except:\n",
    "        pass\n",
    "    return [s.strip().lower() for s in str(x).split(\",\") if s.strip()]\n",
    "\n",
    "df[\"skill_list\"] = df[\"skill_list\"].apply(to_skill_list)\n",
    "df[\"skill_categories\"] = df[\"skill_categories\"].fillna(\"\").astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191351d1",
   "metadata": {},
   "source": [
    "Association Rules: 3 Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b58a43d",
   "metadata": {},
   "source": [
    "Model A1: skills only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ddbd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_skills = df[\"skill_list\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17fb03",
   "metadata": {},
   "source": [
    "Model A2: categories only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26afd025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_list(cats_str):\n",
    "    if not cats_str:\n",
    "        return []\n",
    "    return [c.strip().lower() for c in cats_str.split(\",\") if c.strip()]\n",
    "\n",
    "df[\"cat_list\"] = df[\"skill_categories\"].apply(cat_list)\n",
    "transactions_cats = df[\"cat_list\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd1f4cc",
   "metadata": {},
   "source": [
    "Model A3: skills + categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f168d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_list(row):\n",
    "    return row[\"skill_list\"] + row[\"cat_list\"]\n",
    "\n",
    "df[\"combined_list\"] = df.apply(combined_list, axis=1)\n",
    "transactions_combined = df[\"combined_list\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d33d01",
   "metadata": {},
   "source": [
    "One-hot encode and run Apriori / FP-Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7455f48",
   "metadata": {},
   "source": [
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f304fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_4476\\2700307426.py:32: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  skills_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions: 2,884,556\n",
      "Kept skills: 79\n",
      "Frequent itemsets: 155\n",
      "Rules: 73\n"
     ]
    }
   ],
   "source": [
    "# Replace apriori with a memory-friendly FP-Growth pipeline\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "# --- Parameters you can tune ---\n",
    "min_support = 0.01        # same as your original apriori support\n",
    "min_confidence = 0.4      # same as before\n",
    "min_occurrences = 10      # absolute min occurrences for a skill (safety net)\n",
    "# -------------------------------\n",
    "\n",
    "n_transactions = len(transactions_skills)\n",
    "# derive an absolute min occurrence threshold from min_support (safer)\n",
    "min_occ_from_support = max(1, int(min_support * n_transactions))\n",
    "min_keep = max(min_occurrences, min_occ_from_support)\n",
    "\n",
    "# 1) Count skill frequencies and filter rare skills\n",
    "skill_counts = Counter(skill for tx in transactions_skills for skill in tx)\n",
    "valid_skills = {skill for skill, cnt in skill_counts.items() if cnt >= min_keep}\n",
    "filtered_transactions = [[s for s in tx if s in valid_skills] for tx in transactions_skills]\n",
    "\n",
    "if len(valid_skills) == 0:\n",
    "    raise ValueError(\"No skills left after filtering. Lower min_keep or min_support.\")\n",
    "\n",
    "# 2) Encode transactions as a sparse matrix to save memory\n",
    "te = TransactionEncoder()\n",
    "# fit on filtered transactions (much smaller vocabulary)\n",
    "te_ary = te.fit(filtered_transactions).transform(filtered_transactions, sparse=True)\n",
    "\n",
    "# convert to a pandas SparseDataFrame (saves memory compared to dense)\n",
    "skills_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "# 3) Run FP-Growth (more efficient than Apriori on large data)\n",
    "freq_itemsets_A1 = fpgrowth(skills_df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# 4) Derive association rules\n",
    "rules_A1 = association_rules(freq_itemsets_A1, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "# Optional: quick diagnostics\n",
    "print(f\"Transactions: {n_transactions:,}\")\n",
    "print(f\"Kept skills: {len(valid_skills):,}\")\n",
    "print(f\"Frequent itemsets: {len(freq_itemsets_A1):,}\")\n",
    "print(f\"Rules: {len(rules_A1):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d12db0b",
   "metadata": {},
   "source": [
    "A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9b4c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions_cats).transform(transactions_cats)\n",
    "cats_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "freq_itemsets_A2 = apriori(cats_df, min_support=0.01, use_colnames=True)\n",
    "rules_A2 = association_rules(freq_itemsets_A2, metric=\"confidence\", min_threshold=0.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d23cc983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rules: 17\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rules: {len(rules_A2):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778571bc",
   "metadata": {},
   "source": [
    "A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d521f9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_4476\\3888980484.py:23: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  cats_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions: 2,884,556\n",
      "Kept categories: 84\n",
      "Frequent itemsets: 693\n",
      "Rules: 1,801\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "# Parameters\n",
    "min_support = 0.01\n",
    "min_confidence = 0.4\n",
    "min_occurrences = 10  # remove very rare categories\n",
    "\n",
    "n_transactions = len(transactions_combined)\n",
    "min_occ_from_support = max(1, int(min_support * n_transactions))\n",
    "min_keep = max(min_occurrences, min_occ_from_support)\n",
    "\n",
    "# 1) Filter rare categories\n",
    "cat_counts = Counter(cat for tx in transactions_combined for cat in tx)\n",
    "valid_cats = {cat for cat, cnt in cat_counts.items() if cnt >= min_keep}\n",
    "filtered_transactions = [[c for c in tx if c in valid_cats] for tx in transactions_combined]\n",
    "\n",
    "# 2) Encode as sparse matrix\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(filtered_transactions).transform(filtered_transactions, sparse=True)\n",
    "cats_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "# 3) Use FP-Growth instead of Apriori\n",
    "freq_itemsets_A3 = fpgrowth(cats_df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# 4) Generate rules\n",
    "rules_A3 = association_rules(freq_itemsets_A3, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "# Diagnostics\n",
    "print(f\"Transactions: {n_transactions:,}\")\n",
    "print(f\"Kept categories: {len(valid_cats):,}\")\n",
    "print(f\"Frequent itemsets: {len(freq_itemsets_A3):,}\")\n",
    "print(f\"Rules: {len(rules_A3):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e6891d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===  A1: skill-level  ===\n",
      "n_rules: 73\n",
      "support range: 0.010083007575515954 â†’ 0.0476239670854024\n",
      "confidence range: 0.4012219636893173 â†’ 0.8577052868391452\n",
      "lift range: 3.2580860570026773 â†’ 56.55732135912904\n",
      "\n",
      "===  A2: category-level  ===\n",
      "n_rules: 17\n",
      "support range: 0.01090774455410122 â†’ 0.20140083950528262\n",
      "confidence range: 0.5202852510010257 â†’ 1.0\n",
      "lift range: 1.0008216365247928 â†’ 3.1797653362987273\n",
      "\n",
      "===  A3: combined  ===\n",
      "n_rules: 1801\n",
      "support range: 0.010000499210277075 â†’ 0.20149374808462722\n",
      "confidence range: 0.4012219636893173 â†’ 1.0\n",
      "lift range: 1.0006534533036906 â†’ 56.55732135912904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def summarize_rules(rules, name):\n",
    "    print(\"=== \", name, \" ===\")\n",
    "    print(\"n_rules:\", len(rules))\n",
    "    print(\"support range:\", rules[\"support\"].min(), \"â†’\", rules[\"support\"].max())\n",
    "    print(\"confidence range:\", rules[\"confidence\"].min(), \"â†’\", rules[\"confidence\"].max())\n",
    "    print(\"lift range:\", rules[\"lift\"].min(), \"â†’\", rules[\"lift\"].max())\n",
    "    print()\n",
    "\n",
    "summarize_rules(rules_A1, \"A1: skill-level\")\n",
    "summarize_rules(rules_A2, \"A2: category-level\")\n",
    "summarize_rules(rules_A3, \"A3: combined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947185d8",
   "metadata": {},
   "source": [
    "2ï¸âƒ£ Clustering: 3 Models (KMeans, DBSCAN, Agglomerative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13f880",
   "metadata": {},
   "source": [
    "Step 2.0 â€“ Load your data and import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d00547e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_26700\\1661242774.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"all_jobs_mapped.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "import joblib\n",
    "\n",
    "# Load your cleaned dataset\n",
    "df = pd.read_csv(\"all_jobs_mapped.csv\")\n",
    "\n",
    "# Ensure skill_list is properly formatted as list\n",
    "def to_skill_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(x)\n",
    "        if isinstance(val, list):\n",
    "            return [str(s).strip().lower() for s in val]\n",
    "    except:\n",
    "        pass\n",
    "    return [s.strip().lower() for s in str(x).split(\",\") if s.strip()]\n",
    "\n",
    "df[\"skill_list\"] = df[\"skill_list\"].apply(to_skill_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f9656f",
   "metadata": {},
   "source": [
    "Step 2.1 â€“ Build job Ã— skill matrix (MiniBatch Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b806cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Building global skill vocabulary from ALL data...\n",
      "  Processed 0 jobs...\n",
      "  Processed 100,000 jobs...\n",
      "  Processed 200,000 jobs...\n",
      "  Processed 300,000 jobs...\n",
      "  Processed 400,000 jobs...\n",
      "  Processed 500,000 jobs...\n",
      "  Processed 600,000 jobs...\n",
      "  Processed 700,000 jobs...\n",
      "  Processed 800,000 jobs...\n",
      "  Processed 900,000 jobs...\n",
      "  Processed 1,000,000 jobs...\n",
      "  Processed 1,100,000 jobs...\n",
      "  Processed 1,200,000 jobs...\n",
      "  Processed 1,300,000 jobs...\n",
      "  Processed 1,400,000 jobs...\n",
      "  Processed 1,500,000 jobs...\n",
      "  Processed 1,600,000 jobs...\n",
      "  Processed 1,700,000 jobs...\n",
      "  Processed 1,800,000 jobs...\n",
      "  Processed 1,900,000 jobs...\n",
      "  Processed 2,000,000 jobs...\n",
      "  Processed 2,100,000 jobs...\n",
      "  Processed 2,200,000 jobs...\n",
      "  Processed 2,300,000 jobs...\n",
      "  Processed 2,400,000 jobs...\n",
      "  Processed 2,500,000 jobs...\n",
      "  Processed 2,600,000 jobs...\n",
      "  Processed 2,700,000 jobs...\n",
      "  Processed 2,800,000 jobs...\n",
      "âœ… Global vocabulary: 40234 skills (appearing â‰¥ 50 times)\n",
      "ğŸ”„ Transforming data with FeatureHasher...\n",
      "âœ… Feature matrix built: (2884556, 1000)\n",
      "ğŸ’¾ Memory usage: 176.85 MB\n",
      "Created dense sample: (20000, 1000) for models requiring dense matrices\n"
     ]
    }
   ],
   "source": [
    "def build_minibatch_feature_matrix(df, chunk_size=50000, min_freq=50, n_features=1000):\n",
    "    \"\"\"Build feature matrix using MiniBatch approach for entire dataset\"\"\"\n",
    "    print(\"ğŸ” Building global skill vocabulary from ALL data...\")\n",
    "    \n",
    "    # Step 1: Build vocabulary from entire dataset in chunks\n",
    "    all_skills = Counter()\n",
    "    total_chunks = (len(df) + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        for skills in chunk[\"skill_list\"]:\n",
    "            all_skills.update(skills)\n",
    "        if i % 100000 == 0:\n",
    "            print(f\"  Processed {i:,} jobs...\")\n",
    "    \n",
    "    # Keep most frequent skills\n",
    "    keep_skills = {k for k, v in all_skills.items() if v >= min_freq}\n",
    "    skill_vocab = sorted(list(keep_skills))\n",
    "    \n",
    "    print(f\"âœ… Global vocabulary: {len(skill_vocab)} skills (appearing â‰¥ {min_freq} times)\")\n",
    "    \n",
    "    # Step 2: Use FeatureHasher for memory-efficient transformation\n",
    "    print(\"ğŸ”„ Transforming data with FeatureHasher...\")\n",
    "    \n",
    "    def skills_to_dict(skills_list):\n",
    "        return {skill: 1 for skill in skills_list if skill in keep_skills}\n",
    "    \n",
    "    # Process in chunks to avoid memory issues\n",
    "    hasher = FeatureHasher(n_features=n_features, input_type='dict', alternate_sign=False)\n",
    "    \n",
    "    # Process in chunks and store results\n",
    "    X_chunks = []\n",
    "    \n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        chunk_dicts = chunk[\"skill_list\"].apply(skills_to_dict).tolist()\n",
    "        X_chunk = hasher.transform(chunk_dicts)\n",
    "        X_chunks.append(X_chunk)\n",
    "    \n",
    "    # Combine all chunks\n",
    "    from scipy.sparse import vstack\n",
    "    X_sparse = vstack(X_chunks)\n",
    "    \n",
    "    print(f\"âœ… Feature matrix built: {X_sparse.shape}\")\n",
    "    print(f\"ğŸ’¾ Memory usage: {X_sparse.data.nbytes / (1024**2):.2f} MB\")\n",
    "    \n",
    "    return X_sparse, skill_vocab, hasher\n",
    "\n",
    "# Build the feature matrix for entire dataset\n",
    "X_sparse, skill_vocab, hasher = build_minibatch_feature_matrix(df, chunk_size=50000, min_freq=50, n_features=1000)\n",
    "\n",
    "# Create a smaller dense sample for models that require dense matrices\n",
    "sample_size = min(20000, X_sparse.shape[0])\n",
    "X_dense_sample = X_sparse[:sample_size].toarray()\n",
    "\n",
    "print(f\"Created dense sample: {X_dense_sample.shape} for models requiring dense matrices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1980ac3",
   "metadata": {},
   "source": [
    "Step 2.2 â€“ Model C1: K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f48f1864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans Clustering Results:\n",
      "========================================\n",
      "Running KMeans with k=5...\n",
      "KMeans k=5: silhouette=0.010\n",
      "Running KMeans with k=8...\n",
      "KMeans k=8: silhouette=0.007\n",
      "Running KMeans with k=10...\n",
      "KMeans k=10: silhouette=0.005\n",
      "\n",
      "Best KMeans: k=5 (silhouette=0.010)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "k_values = [5, 8, 10]  # Reduced for faster computation\n",
    "results_kmeans = {}\n",
    "\n",
    "print(\"KMeans Clustering Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"Running KMeans with k={k}...\")\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=3)\n",
    "    \n",
    "    # KMeans works with sparse matrices, so use larger sample\n",
    "    kmeans_sample_size = min(50000, X_sparse.shape[0])\n",
    "    X_kmeans = X_sparse[:kmeans_sample_size]\n",
    "    \n",
    "    labels = km.fit_predict(X_kmeans)\n",
    "    results_kmeans[k] = labels\n",
    "    \n",
    "    # Evaluate with silhouette score\n",
    "    score = silhouette_score(X_kmeans, labels)\n",
    "    results_kmeans[f\"silhouette_{k}\"] = score\n",
    "    print(f\"KMeans k={k}: silhouette={score:.3f}\")\n",
    "\n",
    "# Find best K\n",
    "best_k_kmeans = max(k_values, key=lambda k: results_kmeans[f\"silhouette_{k}\"])\n",
    "print(f\"\\nBest KMeans: k={best_k_kmeans} (silhouette={results_kmeans[f'silhouette_{best_k_kmeans}']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ab63d1",
   "metadata": {},
   "source": [
    "Step 2.3 â€“ Model C2: DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7bd2e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN running on 20000 samples\n",
      "DBSCAN Clustering Results:\n",
      "========================================\n",
      "Testing DBSCAN eps=0.5, min_samples=10...\n",
      "DBSCAN eps=0.5, min_samples=10:\n",
      "  Clusters: 4, Noise: 19800 (99.0%)\n",
      "  Silhouette: 1.000\n",
      "\n",
      "Testing DBSCAN eps=0.5, min_samples=15...\n",
      "DBSCAN eps=0.5, min_samples=15:\n",
      "  Clusters: 2, Noise: 19824 (99.1%)\n",
      "  Silhouette: 1.000\n",
      "\n",
      "Testing DBSCAN eps=0.7, min_samples=10...\n",
      "DBSCAN eps=0.7, min_samples=10:\n",
      "  Clusters: 4, Noise: 19800 (99.0%)\n",
      "  Silhouette: 1.000\n",
      "\n",
      "Testing DBSCAN eps=0.7, min_samples=15...\n",
      "DBSCAN eps=0.7, min_samples=15:\n",
      "  Clusters: 2, Noise: 19824 (99.1%)\n",
      "  Silhouette: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# DBSCAN works with sparse matrices\n",
    "dbscan_sample_size = min(20000, X_sparse.shape[0])\n",
    "X_dbscan = X_sparse[:dbscan_sample_size]\n",
    "\n",
    "print(f\"DBSCAN running on {X_dbscan.shape[0]} samples\")\n",
    "print(\"DBSCAN Clustering Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "eps_values = [0.5, 0.7]\n",
    "min_samples_values = [10, 15]\n",
    "\n",
    "results_dbscan = {}\n",
    "\n",
    "for eps in eps_values:\n",
    "    for ms in min_samples_values:\n",
    "        print(f\"Testing DBSCAN eps={eps}, min_samples={ms}...\")\n",
    "        db = DBSCAN(eps=eps, min_samples=ms, metric='euclidean', n_jobs=-1)\n",
    "        labels = db.fit_predict(X_dbscan)\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = (labels == -1).sum()\n",
    "        noise_percentage = (n_noise / len(labels)) * 100\n",
    "        \n",
    "        # Store results\n",
    "        key = f\"eps{eps}_min{ms}\"\n",
    "        results_dbscan[key] = {\n",
    "            'labels': labels,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'noise_percentage': noise_percentage\n",
    "        }\n",
    "        \n",
    "        print(f\"DBSCAN eps={eps}, min_samples={ms}:\")\n",
    "        print(f\"  Clusters: {n_clusters}, Noise: {n_noise} ({noise_percentage:.1f}%)\")\n",
    "        \n",
    "        # silhouette only if >1 cluster and not all noise\n",
    "        if n_clusters > 1 and n_clusters < len(labels):\n",
    "            mask = labels != -1\n",
    "            if len(set(labels[mask])) > 1 and sum(mask) > 1:\n",
    "                score = silhouette_score(X_dbscan[mask], labels[mask])\n",
    "                results_dbscan[key]['silhouette'] = score\n",
    "                print(f\"  Silhouette: {score:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcd4e38",
   "metadata": {},
   "source": [
    "Step 2.4 â€“ Model C3: Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ada9918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agglomerative Clustering Results:\n",
      "========================================\n",
      "Running Agglomerative with k=5...\n",
      "Agglomerative k=5: silhouette=-0.047\n",
      "Running Agglomerative with k=8...\n",
      "Agglomerative k=8: silhouette=-0.044\n",
      "Running Agglomerative with k=10...\n",
      "Agglomerative k=10: silhouette=-0.053\n",
      "\n",
      "Best Agglomerative: k=8 (silhouette=-0.044)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "print(\"Agglomerative Clustering Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "k_values = [5, 8, 10]\n",
    "results_agglo = {}\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"Running Agglomerative with k={k}...\")\n",
    "    ac = AgglomerativeClustering(n_clusters=k)\n",
    "    \n",
    "    # Agglomerative requires dense matrix, so use our dense sample\n",
    "    labels = ac.fit_predict(X_dense_sample)\n",
    "    results_agglo[k] = labels\n",
    "    \n",
    "    score = silhouette_score(X_dense_sample, labels)\n",
    "    results_agglo[f\"silhouette_{k}\"] = score\n",
    "    print(f\"Agglomerative k={k}: silhouette={score:.3f}\")\n",
    "\n",
    "# Find best K for Agglomerative\n",
    "best_k_agglo = max(k_values, key=lambda k: results_agglo[f\"silhouette_{k}\"])\n",
    "print(f\"\\nBest Agglomerative: k={best_k_agglo} (silhouette={results_agglo[f'silhouette_{best_k_agglo}']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e16c4",
   "metadata": {},
   "source": [
    "Step 2.5 â€“ Compare clustering models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f342bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTERING MODEL COMPARISON\n",
      "==================================================\n",
      "KMeans Results:\n",
      "  k=5: silhouette=0.010\n",
      "  k=8: silhouette=0.007\n",
      "  k=10: silhouette=0.005\n",
      "\n",
      "Agglomerative Results:\n",
      "  k=5: silhouette=-0.047\n",
      "  k=8: silhouette=-0.044\n",
      "  k=10: silhouette=-0.053\n",
      "\n",
      "DBSCAN Results:\n",
      "  eps0.5_min10: silhouette=1.000 (clusters: 4, noise: 99.0%)\n",
      "  eps0.5_min15: silhouette=1.000 (clusters: 2, noise: 99.1%)\n",
      "  eps0.7_min10: silhouette=1.000 (clusters: 4, noise: 99.0%)\n",
      "  eps0.7_min15: silhouette=1.000 (clusters: 2, noise: 99.1%)\n",
      "==================================================\n",
      "ğŸ† BEST MODEL: DBSCAN\n",
      "ğŸ“Š Best Configuration: eps0.5_min10\n",
      "ğŸ¯ Best Silhouette Score: 1.000\n"
     ]
    }
   ],
   "source": [
    "def compare_all_models(results_kmeans, results_dbscan, results_agglo, k_values):\n",
    "    \"\"\"Compare all clustering models and select the best one\"\"\"\n",
    "    \n",
    "    print(\"CLUSTERING MODEL COMPARISON\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    best_model = None\n",
    "    best_score = -1\n",
    "    best_config = None\n",
    "    \n",
    "    # Compare KMeans models\n",
    "    print(\"KMeans Results:\")\n",
    "    for k in k_values:\n",
    "        score = results_kmeans[f\"silhouette_{k}\"]\n",
    "        print(f\"  k={k}: silhouette={score:.3f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model = \"KMeans\"\n",
    "            best_config = k\n",
    "    \n",
    "    # Compare Agglomerative models  \n",
    "    print(\"\\nAgglomerative Results:\")\n",
    "    for k in k_values:\n",
    "        score = results_agglo[f\"silhouette_{k}\"]\n",
    "        print(f\"  k={k}: silhouette={score:.3f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model = \"Agglomerative\"\n",
    "            best_config = k\n",
    "    \n",
    "    # Compare DBSCAN models\n",
    "    print(\"\\nDBSCAN Results:\")\n",
    "    for key, result in results_dbscan.items():\n",
    "        if 'silhouette' in result:\n",
    "            score = result['silhouette']\n",
    "            print(f\"  {key}: silhouette={score:.3f} (clusters: {result['n_clusters']}, noise: {result['noise_percentage']:.1f}%)\")\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = \"DBSCAN\"\n",
    "                best_config = key\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ† BEST MODEL: {best_model}\")\n",
    "    print(f\"ğŸ“Š Best Configuration: {best_config}\")\n",
    "    print(f\"ğŸ¯ Best Silhouette Score: {best_score:.3f}\")\n",
    "    \n",
    "    return best_model, best_config, best_score\n",
    "\n",
    "# Run comparison\n",
    "best_model, best_config, best_score = compare_all_models(results_kmeans, results_dbscan, results_agglo, k_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f2953",
   "metadata": {},
   "source": [
    "Step 2.6 â€“ Apply Best Model to Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd892795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ STEP 2.6: Applying OPTIMIZED DBSCAN to FULL DATASET...\n",
      "==================================================\n",
      "Training DBSCAN on FULL dataset with OPTIMIZED parameters:\n",
      "  eps: 0.7, min_samples: 50\n",
      "  Dataset size: 2,884,556 jobs\n",
      "  This will take approximately 30-90 minutes...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m start_time = time.time()\n\u001b[32m     18\u001b[39m dbscan_full = DBSCAN(eps=eps, min_samples=min_samples, metric=\u001b[33m'\u001b[39m\u001b[33meuclidean\u001b[39m\u001b[33m'\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m full_labels = \u001b[43mdbscan_full\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sparse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m end_time = time.time()\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… DBSCAN completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time)/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\cluster\\_dbscan.py:473\u001b[39m, in \u001b[36mDBSCAN.fit_predict\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    449\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute clusters from a data or distance matrix and predict labels.\u001b[39;00m\n\u001b[32m    450\u001b[39m \n\u001b[32m    451\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    471\u001b[39m \u001b[33;03m        Cluster labels. Noisy samples are given the label -1.\u001b[39;00m\n\u001b[32m    472\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.labels_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\cluster\\_dbscan.py:421\u001b[39m, in \u001b[36mDBSCAN.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    419\u001b[39m neighbors_model.fit(X)\n\u001b[32m    420\u001b[39m \u001b[38;5;66;03m# This has worst case O(n^2) memory complexity\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m neighborhoods = \u001b[43mneighbors_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mradius_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    424\u001b[39m     n_neighbors = np.array([\u001b[38;5;28mlen\u001b[39m(neighbors) \u001b[38;5;28;01mfor\u001b[39;00m neighbors \u001b[38;5;129;01min\u001b[39;00m neighborhoods])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:1258\u001b[39m, in \u001b[36mRadiusNeighborsMixin.radius_neighbors\u001b[39m\u001b[34m(self, X, radius, return_distance, sort_results)\u001b[39m\n\u001b[32m   1256\u001b[39m     results = neigh_dist, neigh_ind\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1258\u001b[39m     neigh_ind_list = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunked_results\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1259\u001b[39m     results = _to_object_array(neigh_ind_list)\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sort_results:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2240\u001b[39m, in \u001b[36mpairwise_distances_chunked\u001b[39m\u001b[34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[39m\n\u001b[32m   2238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2239\u001b[39m     X_chunk = X[sl]\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m D_chunk = \u001b[43mpairwise_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS.get(\n\u001b[32m   2242\u001b[39m     metric, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2243\u001b[39m ) \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[32m   2244\u001b[39m     \u001b[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[32m   2245\u001b[39m     \u001b[38;5;66;03m# i.e. \"l2\"\u001b[39;00m\n\u001b[32m   2246\u001b[39m     D_chunk.flat[sl.start :: _num_samples(X) + \u001b[32m1\u001b[39m] = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2476\u001b[39m, in \u001b[36mpairwise_distances\u001b[39m\u001b[34m(X, Y, metric, n_jobs, force_all_finite, ensure_all_finite, **kwds)\u001b[39m\n\u001b[32m   2473\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m distance.squareform(distance.pdist(X, metric=metric, **kwds))\n\u001b[32m   2474\u001b[39m     func = partial(distance.cdist, metric=metric, **kwds)\n\u001b[32m-> \u001b[39m\u001b[32m2476\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_pairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1965\u001b[39m, in \u001b[36m_parallel_pairwise\u001b[39m\u001b[34m(X, Y, func, n_jobs, **kwds)\u001b[39m\n\u001b[32m   1963\u001b[39m fd = delayed(_dist_wrapper)\n\u001b[32m   1964\u001b[39m ret = np.empty((X.shape[\u001b[32m0\u001b[39m], Y.shape[\u001b[32m0\u001b[39m]), dtype=dtype, order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1965\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreading\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1967\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_num_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_n_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[32m   1971\u001b[39m     \u001b[38;5;66;03m# zeroing diagonal for euclidean norm.\u001b[39;00m\n\u001b[32m   1972\u001b[39m     \u001b[38;5;66;03m# TODO: do it also for other norms.\u001b[39;00m\n\u001b[32m   1973\u001b[39m     np.fill_diagonal(ret, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# STEP 2.6 â€“ Apply DBSCAN to Full Dataset (OPTIMIZED VERSION)\n",
    "print(\"ğŸš€ STEP 2.6: Applying OPTIMIZED DBSCAN to FULL DATASET...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use more aggressive parameters for large dataset\n",
    "eps = 0.7  # Reduced from 2.0 - tighter neighborhoods\n",
    "min_samples = 50  # Increased from 10 - require more points to form clusters\n",
    "\n",
    "print(f\"Training DBSCAN on FULL dataset with OPTIMIZED parameters:\")\n",
    "print(f\"  eps: {eps}, min_samples: {min_samples}\")\n",
    "print(f\"  Dataset size: {X_sparse.shape[0]:,} jobs\")\n",
    "print(f\"  This will take approximately 30-90 minutes...\")\n",
    "\n",
    "# Train DBSCAN with progress tracking\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "dbscan_full = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean', n_jobs=-1)\n",
    "full_labels = dbscan_full.fit_predict(X_sparse)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"âœ… DBSCAN completed in {(end_time - start_time)/60:.1f} minutes\")\n",
    "\n",
    "# Add clusters to the main dataframe\n",
    "df_full = df.copy()\n",
    "df_full['cluster'] = full_labels\n",
    "\n",
    "# Calculate statistics\n",
    "n_clusters = len(set(full_labels)) - (1 if -1 in full_labels else 0)\n",
    "n_noise = (full_labels == -1).sum()\n",
    "clustered_percentage = ((len(full_labels) - n_noise) / len(full_labels)) * 100\n",
    "\n",
    "print(f\"ğŸ“Š Results:\")\n",
    "print(f\"  Total jobs: {len(full_labels):,}\")\n",
    "print(f\"  Number of clusters: {n_clusters}\")\n",
    "print(f\"  Noise points: {n_noise:,} ({n_noise/len(full_labels)*100:.1f}%)\")\n",
    "print(f\"  Clustered points: {len(full_labels)-n_noise:,} ({clustered_percentage:.1f}%)\")\n",
    "\n",
    "# Save results\n",
    "df_full.to_csv(\"all_jobs_clustered_full.csv\", index=False)\n",
    "print(f\"âœ… Full dataset saved: all_jobs_clustered_full.csv\")\n",
    "\n",
    "joblib.dump({\n",
    "    'best_model': 'DBSCAN',\n",
    "    'parameters': {'eps': eps, 'min_samples': min_samples},\n",
    "    'skill_vocab': skill_vocab,\n",
    "    'feature_hasher': hasher,\n",
    "    'trained_model': dbscan_full,\n",
    "}, 'clustering_results.pkl')\n",
    "\n",
    "print(\"âœ… Model and metadata saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
