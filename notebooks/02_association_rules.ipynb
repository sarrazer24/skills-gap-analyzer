{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13934629,"sourceType":"datasetVersion","datasetId":8879404}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sarraverse/02-association-rules?scriptVersionId=284138306\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ASSOCIATION RULES MINING (3 MODELS)\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport ast\nfrom collections import Counter\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import fpgrowth, apriori, association_rules\n\n# Load skills dictionary\nskills_meta = pd.read_csv(\"/kaggle/input/job-skills/skill_migration_clean.csv\")\n\nskill_dict = (\n    skills_meta[[\"skill_group_name\", \"skill_group_category\"]]\n    .drop_duplicates()\n    .set_index(\"skill_group_name\")[\"skill_group_category\"]\n    .to_dict()\n)\n\nprint(f\"Loaded {len(skill_dict)} skills.\")\n\n# Load cleaned jobs data\npath = \"/kaggle/input/job-skills/all_jobs_mapped.csv\"\ndf = pd.read_csv(path, nrows=200000)\nprint(f\"Loaded dataset shape: {df.shape}\")\n\n# Ensure skill_list is properly formatted as list\ndef to_skill_list(x):\n    if isinstance(x, list):\n        return x\n    if pd.isna(x):\n        return []\n    try:\n        val = ast.literal_eval(x)\n        if isinstance(val, list):\n            return [str(s).strip().lower() for s in val]\n    except:\n        pass\n    return [s.strip().lower() for s in str(x).split(\",\") if s.strip()]\n\ndf[\"skill_list\"] = df[\"skill_list\"].apply(to_skill_list)\ndf[\"skill_categories\"] = df[\"skill_categories\"].fillna(\"\").astype(str)\n\n# Define helper functions for parsing skills\ndef parse_skills(raw):\n    if pd.isna(raw):\n        return []\n    text = str(raw).strip().lower()\n    if not text:\n        return []\n    parts = re.split(r\",|/|;|\\||\\+\", text)\n    return [p.strip() for p in parts if p.strip()]\n\ndef map_skill_categories(skill_list):\n    cats = set()\n    for s in skill_list:\n        cat = skill_dict.get(s)\n        if cat is None:\n            cat = \"other\"\n        cats.add(cat)\n    if not cats:\n        return \"\"\n    return \",\".join(sorted(cats))\n\ndef cat_list(cats_str):\n    if not cats_str:\n        return []\n    return [c.strip().lower() for c in cats_str.split(\",\") if c.strip()]\n\n# Prepare transactions for all models\nprint(\"\\nPreparing transaction data...\")\n\n# Model A1: skills only\ntransactions_skills = df[\"skill_list\"].tolist()\nprint(f\"Model A1: {len(transactions_skills)} transactions with skills\")\n\n# Model A2: categories only\ndf[\"cat_list\"] = df[\"skill_categories\"].apply(cat_list)\ntransactions_cats = df[\"cat_list\"].tolist()\nprint(f\"Model A2: {len(transactions_cats)} transactions with categories\")\n\n# Model A3: skills + categories\ndef combined_list(row):\n    return row[\"skill_list\"] + row[\"cat_list\"]\n\ndf[\"combined_list\"] = df.apply(combined_list, axis=1)\ntransactions_combined = df[\"combined_list\"].tolist()\nprint(f\"Model A3: {len(transactions_combined)} transactions with skills + categories\")\n\n# Model A1: FP-Growth with skill-level rules\nprint(\"\\n=== Running Model A1: Skill-Level Association Rules ===\")\nn_transactions = len(transactions_skills)\nmin_support = 0.01\nmin_confidence = 0.4\nmin_occurrences = 10\n\nmin_occ_from_support = max(1, int(min_support * n_transactions))\nmin_keep = max(min_occurrences, min_occ_from_support)\n\n# 1) Count skill frequencies and filter rare skills\nskill_counts = Counter(skill for tx in transactions_skills for skill in tx)\nvalid_skills = {skill for skill, cnt in skill_counts.items() if cnt >= min_keep}\nfiltered_transactions = [[s for s in tx if s in valid_skills] for tx in transactions_skills]\n\nprint(f\"Kept skills: {len(valid_skills):,} (â‰¥ {min_keep} occurrences)\")\n\n# 2) Encode transactions as a sparse matrix\nte = TransactionEncoder()\nte_ary = te.fit(filtered_transactions).transform(filtered_transactions, sparse=True)\nskills_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n\n# 3) Run FP-Growth\nfreq_itemsets_A1 = fpgrowth(skills_df, min_support=min_support, use_colnames=True)\n\n# 4) Derive association rules\nrules_A1 = association_rules(freq_itemsets_A1, metric=\"confidence\", min_threshold=min_confidence)\n\nprint(f\"âœ… Model A1: {len(rules_A1):,} rules found\")\n\n# Model A2: Category-level rules\nprint(\"\\n=== Running Model A2: Category-Level Association Rules ===\")\nte = TransactionEncoder()\nte_ary = te.fit(transactions_cats).transform(transactions_cats)\ncats_df = pd.DataFrame(te_ary, columns=te.columns_)\n\nfreq_itemsets_A2 = apriori(cats_df, min_support=0.01, use_colnames=True)\nrules_A2 = association_rules(freq_itemsets_A2, metric=\"confidence\", min_threshold=0.4)\n\nprint(f\"âœ… Model A2: {len(rules_A2):,} rules found\")\n\n# Model A3: Combined rules\nprint(\"\\n=== Running Model A3: Combined Association Rules ===\")\nn_transactions = len(transactions_combined)\nmin_support = 0.01\nmin_confidence = 0.4\nmin_occurrences = 10\n\nmin_occ_from_support = max(1, int(min_support * n_transactions))\nmin_keep = max(min_occurrences, min_occ_from_support)\n\n# 1) Filter rare categories\ncat_counts = Counter(cat for tx in transactions_combined for cat in tx)\nvalid_cats = {cat for cat, cnt in cat_counts.items() if cnt >= min_keep}\nfiltered_transactions = [[c for c in tx if c in valid_cats] for tx in transactions_combined]\n\nprint(f\"Kept categories: {len(valid_cats):,} (â‰¥ {min_keep} occurrences)\")\n\n# 2) Encode as sparse matrix\nte = TransactionEncoder()\nte_ary = te.fit(filtered_transactions).transform(filtered_transactions, sparse=True)\ncats_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n\n# 3) Use FP-Growth\nfreq_itemsets_A3 = fpgrowth(cats_df, min_support=min_support, use_colnames=True)\n\n# 4) Generate rules\nrules_A3 = association_rules(freq_itemsets_A3, metric=\"confidence\", min_threshold=min_confidence)\n\nprint(f\"âœ… Model A3: {len(rules_A3):,} rules found\")\n\n# Summary function\ndef summarize_rules(rules, name):\n    print(f\"\\n=== {name} ===\")\n    print(f\"Number of rules: {len(rules):,}\")\n    print(f\"Support range: {rules['support'].min():.4f} â†’ {rules['support'].max():.4f}\")\n    print(f\"Confidence range: {rules['confidence'].min():.4f} â†’ {rules['confidence'].max():.4f}\")\n    print(f\"Lift range: {rules['lift'].min():.4f} â†’ {rules['lift'].max():.4f}\")\n    \n    # Show top 5 rules by confidence\n    if len(rules) > 0:\n        print(\"\\nTop 5 rules by confidence:\")\n        top_rules = rules.sort_values('confidence', ascending=False).head(5)\n        for idx, row in top_rules.iterrows():\n            print(f\"  {set(row['antecedents'])} â†’ {set(row['consequents'])}\")\n            print(f\"    Support: {row['support']:.4f}, Confidence: {row['confidence']:.4f}, Lift: {row['lift']:.4f}\")\n\n# Summarize all models\nsummarize_rules(rules_A1, \"A1: skill-level\")\nsummarize_rules(rules_A2, \"A2: category-level\")\nsummarize_rules(rules_A3, \"A3: combined\")\n\n# Save results\nprint(\"\\nðŸ’¾ Saving association rules results...\")\nrules_A1.to_csv(\"/kaggle/working/association_rules_skills.csv\", index=False)\nrules_A2.to_csv(\"/kaggle/working/association_rules_categories.csv\", index=False)\nrules_A3.to_csv(\"/kaggle/working/association_rules_combined.csv\", index=False)\n\nprint(\"âœ… All association rules saved to files!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}