{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/code/sarraverse/02-association-rules?scriptVersionId=284547284\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b74a875",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/code/sarraverse/02-association-rules?scriptVersionId=284161070\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b321591",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-07T19:23:29.461861Z",
     "iopub.status.busy": "2025-12-07T19:23:29.461442Z",
     "iopub.status.idle": "2025-12-07T19:23:31.092868Z",
     "shell.execute_reply": "2025-12-07T19:23:31.09207Z",
     "shell.execute_reply.started": "2025-12-07T19:23:29.461834Z"
    },
    "papermill": {
     "duration": 1.724677,
     "end_time": "2025-12-05T22:21:50.056028",
     "exception": false,
     "start_time": "2025-12-05T22:21:48.331351",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/job-skills/skill_migration_public.csv\n",
      "/kaggle/input/job-skills/all_jobs_clustered_full.csv\n",
      "/kaggle/input/job-skills/association_rules_categories.csv\n",
      "/kaggle/input/job-skills/all_jobs_mapped.csv\n",
      "/kaggle/input/job-skills/all_jobs_clustered_sample_dbscan.csv\n",
      "/kaggle/input/job-skills/association_rules_combined.csv\n",
      "/kaggle/input/job-skills/all_jobs_clean_full.csv\n",
      "/kaggle/input/job-skills/skill_migration_clean.csv\n",
      "/kaggle/input/job-skills/association_rules_skills.csv\n",
      "/kaggle/input/job-skills/all_jobs.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8cda44",
   "metadata": {
    "papermill": {
     "duration": 0.001506,
     "end_time": "2025-12-05T22:21:50.059633",
     "exception": false,
     "start_time": "2025-12-05T22:21:50.058127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ASSOCIATION RULES MINING (3 MODELS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848cc088",
   "metadata": {},
   "source": [
    "## Model Export\n",
    "\n",
    "Export the trained models as pickle files for use in the Streamlit application. **Model A2 (category-level) is used for skill recommendations in the app.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dde1410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T22:21:50.064498Z",
     "iopub.status.busy": "2025-12-05T22:21:50.064076Z",
     "iopub.status.idle": "2025-12-05T22:49:02.792016Z",
     "shell.execute_reply": "2025-12-05T22:49:02.790342Z"
    },
    "papermill": {
     "duration": 1632.734698,
     "end_time": "2025-12-05T22:49:02.795741",
     "exception": false,
     "start_time": "2025-12-05T22:21:50.061043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 249 skills.\n",
      "Loaded dataset shape: (200000, 9)\n",
      "\n",
      "Preparing transaction data...\n",
      "Model A1: 200000 transactions with skills\n",
      "Model A2: 200000 transactions with categories\n",
      "Model A3: 200000 transactions with skills + categories\n",
      "\n",
      "=== Running Model A1: Skill-Level Association Rules ===\n",
      "Kept skills: 171 (‚â• 2000 occurrences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/2373446715.py:108: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  skills_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model A1: 308 rules found\n",
      "\n",
      "=== Running Model A2: Category-Level Association Rules ===\n",
      "‚úÖ Model A2: 22 rules found\n",
      "\n",
      "=== Running Model A3: Combined Association Rules ===\n",
      "Kept categories: 176 (‚â• 2000 occurrences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/2373446715.py:149: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  cats_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model A3: 7,147 rules found\n",
      "\n",
      "=== A1: skill-level ===\n",
      "Number of rules: 308\n",
      "Support range: 0.0101 ‚Üí 0.1013\n",
      "Confidence range: 0.4010 ‚Üí 0.9297\n",
      "Lift range: 1.3813 ‚Üí 41.6524\n",
      "\n",
      "Top 5 rules by confidence:\n",
      "  {'word'} ‚Üí {'excel'}\n",
      "    Support: 0.0112, Confidence: 0.9297, Lift: 27.6909\n",
      "  {'customer service', 'leadership', 'problemsolving'} ‚Üí {'communication'}\n",
      "    Support: 0.0105, Confidence: 0.8965, Lift: 3.0875\n",
      "  {'verbal communication'} ‚Üí {'written communication'}\n",
      "    Support: 0.0196, Confidence: 0.8697, Lift: 35.0674\n",
      "  {'inclusion'} ‚Üí {'diversity'}\n",
      "    Support: 0.0143, Confidence: 0.8685, Lift: 41.6524\n",
      "  {'problem solving', 'customer service', 'leadership'} ‚Üí {'communication'}\n",
      "    Support: 0.0118, Confidence: 0.8664, Lift: 2.9841\n",
      "\n",
      "=== A2: category-level ===\n",
      "Number of rules: 22\n",
      "Support range: 0.0136 ‚Üí 0.4745\n",
      "Confidence range: 0.4756 ‚Üí 1.0000\n",
      "Lift range: 1.0017 ‚Üí 1.2677\n",
      "\n",
      "Top 5 rules by confidence:\n",
      "  {'business skills'} ‚Üí {'other'}\n",
      "    Support: 0.1904, Confidence: 1.0000, Lift: 1.0023\n",
      "  {'soft skills', 'business skills'} ‚Üí {'other'}\n",
      "    Support: 0.1118, Confidence: 1.0000, Lift: 1.0023\n",
      "  {'soft skills', 'specialized industry skills', 'business skills'} ‚Üí {'other'}\n",
      "    Support: 0.0281, Confidence: 1.0000, Lift: 1.0023\n",
      "  {'soft skills', 'tech skills'} ‚Üí {'other'}\n",
      "    Support: 0.0136, Confidence: 1.0000, Lift: 1.0023\n",
      "  {'soft skills', 'specialized industry skills'} ‚Üí {'other'}\n",
      "    Support: 0.1381, Confidence: 1.0000, Lift: 1.0023\n",
      "\n",
      "=== A3: combined ===\n",
      "Number of rules: 7,147\n",
      "Support range: 0.0100 ‚Üí 0.4748\n",
      "Confidence range: 0.4010 ‚Üí 1.0000\n",
      "Lift range: 0.8483 ‚Üí 41.6524\n",
      "\n",
      "Top 5 rules by confidence:\n",
      "  {'pension scheme'} ‚Üí {'other'}\n",
      "    Support: 0.0101, Confidence: 1.0000, Lift: 1.0023\n",
      "  {'teamwork', 'communication', 'time management'} ‚Üí {'other'}\n",
      "    Support: 0.0330, Confidence: 1.0000, Lift: 1.0023\n",
      "  {'communication', 'time management', 'other', 'problemsolving'} ‚Üí {'soft skills'}\n",
      "    Support: 0.0224, Confidence: 1.0000, Lift: 2.1060\n",
      "  {'teamwork', 'time management'} ‚Üí {'soft skills', 'other'}\n",
      "    Support: 0.0487, Confidence: 1.0000, Lift: 2.1062\n",
      "  {'communication', 'time management', 'problemsolving'} ‚Üí {'soft skills', 'other'}\n",
      "    Support: 0.0224, Confidence: 1.0000, Lift: 2.1062\n",
      "\n",
      "üíæ Saving association rules results...\n",
      "‚úÖ All association rules saved to files!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "from collections import Counter\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, apriori, association_rules\n",
    "\n",
    "# Load skills dictionary\n",
    "skills_meta = pd.read_csv(\"/kaggle/input/job-skills/skill_migration_clean.csv\")\n",
    "\n",
    "skill_dict = (\n",
    "    skills_meta[[\"skill_group_name\", \"skill_group_category\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"skill_group_name\")[\"skill_group_category\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(skill_dict)} skills.\")\n",
    "\n",
    "# Load cleaned jobs data\n",
    "path = \"/kaggle/input/job-skills/all_jobs_mapped.csv\"\n",
    "df = pd.read_csv(path, nrows=200000)\n",
    "print(f\"Loaded dataset shape: {df.shape}\")\n",
    "\n",
    "# Ensure skill_list is properly formatted as list\n",
    "def to_skill_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(x)\n",
    "        if isinstance(val, list):\n",
    "            return [str(s).strip().lower() for s in val]\n",
    "    except:\n",
    "        pass\n",
    "    return [s.strip().lower() for s in str(x).split(\",\") if s.strip()]\n",
    "\n",
    "df[\"skill_list\"] = df[\"skill_list\"].apply(to_skill_list)\n",
    "df[\"skill_categories\"] = df[\"skill_categories\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Define helper functions for parsing skills\n",
    "def parse_skills(raw):\n",
    "    if pd.isna(raw):\n",
    "        return []\n",
    "    text = str(raw).strip().lower()\n",
    "    if not text:\n",
    "        return []\n",
    "    parts = re.split(r\",|/|;|\\||\\+\", text)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def map_skill_categories(skill_list):\n",
    "    cats = set()\n",
    "    for s in skill_list:\n",
    "        cat = skill_dict.get(s)\n",
    "        if cat is None:\n",
    "            cat = \"other\"\n",
    "        cats.add(cat)\n",
    "    if not cats:\n",
    "        return \"\"\n",
    "    return \",\".join(sorted(cats))\n",
    "\n",
    "def cat_list(cats_str):\n",
    "    if not cats_str:\n",
    "        return []\n",
    "    return [c.strip().lower() for c in cats_str.split(\",\") if c.strip()]\n",
    "\n",
    "# Prepare transactions for all models\n",
    "print(\"\\nPreparing transaction data...\")\n",
    "\n",
    "# Model A1: skills only\n",
    "transactions_skills = df[\"skill_list\"].tolist()\n",
    "print(f\"Model A1: {len(transactions_skills)} transactions with skills\")\n",
    "\n",
    "# Model A2: categories only\n",
    "df[\"cat_list\"] = df[\"skill_categories\"].apply(cat_list)\n",
    "transactions_cats = df[\"cat_list\"].tolist()\n",
    "print(f\"Model A2: {len(transactions_cats)} transactions with categories\")\n",
    "\n",
    "# Model A3: skills + categories\n",
    "def combined_list(row):\n",
    "    return row[\"skill_list\"] + row[\"cat_list\"]\n",
    "\n",
    "df[\"combined_list\"] = df.apply(combined_list, axis=1)\n",
    "transactions_combined = df[\"combined_list\"].tolist()\n",
    "print(f\"Model A3: {len(transactions_combined)} transactions with skills + categories\")\n",
    "\n",
    "# Model A1: FP-Growth with skill-level rules\n",
    "print(\"\\n=== Running Model A1: Skill-Level Association Rules ===\")\n",
    "n_transactions = len(transactions_skills)\n",
    "min_support = 0.01\n",
    "min_confidence = 0.4\n",
    "min_occurrences = 10\n",
    "\n",
    "min_occ_from_support = max(1, int(min_support * n_transactions))\n",
    "min_keep = max(min_occurrences, min_occ_from_support)\n",
    "\n",
    "# 1) Count skill frequencies and filter rare skills\n",
    "skill_counts = Counter(skill for tx in transactions_skills for skill in tx)\n",
    "valid_skills = {skill for skill, cnt in skill_counts.items() if cnt >= min_keep}\n",
    "filtered_transactions = [[s for s in tx if s in valid_skills] for tx in transactions_skills]\n",
    "\n",
    "print(f\"Kept skills: {len(valid_skills):,} (‚â• {min_keep} occurrences)\")\n",
    "\n",
    "# 2) Encode transactions as a sparse matrix\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(filtered_transactions).transform(filtered_transactions, sparse=True)\n",
    "skills_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "# 3) Run FP-Growth\n",
    "freq_itemsets_A1 = fpgrowth(skills_df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# 4) Derive association rules\n",
    "rules_A1 = association_rules(freq_itemsets_A1, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "print(f\"‚úÖ Model A1: {len(rules_A1):,} rules found\")\n",
    "\n",
    "# Model A2: Category-level rules\n",
    "print(\"\\n=== Running Model A2: Category-Level Association Rules ===\")\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions_cats).transform(transactions_cats)\n",
    "cats_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "freq_itemsets_A2 = apriori(cats_df, min_support=0.01, use_colnames=True)\n",
    "rules_A2 = association_rules(freq_itemsets_A2, metric=\"confidence\", min_threshold=0.4)\n",
    "\n",
    "print(f\"‚úÖ Model A2: {len(rules_A2):,} rules found\")\n",
    "\n",
    "# Model A3: Combined rules\n",
    "print(\"\\n=== Running Model A3: Combined Association Rules ===\")\n",
    "n_transactions = len(transactions_combined)\n",
    "min_support = 0.01\n",
    "min_confidence = 0.4\n",
    "min_occurrences = 10\n",
    "\n",
    "min_occ_from_support = max(1, int(min_support * n_transactions))\n",
    "min_keep = max(min_occurrences, min_occ_from_support)\n",
    "\n",
    "# 1) Filter rare categories\n",
    "cat_counts = Counter(cat for tx in transactions_combined for cat in tx)\n",
    "valid_cats = {cat for cat, cnt in cat_counts.items() if cnt >= min_keep}\n",
    "filtered_transactions = [[c for c in tx if c in valid_cats] for tx in transactions_combined]\n",
    "\n",
    "print(f\"Kept categories: {len(valid_cats):,} (‚â• {min_keep} occurrences)\")\n",
    "\n",
    "# 2) Encode as sparse matrix\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(filtered_transactions).transform(filtered_transactions, sparse=True)\n",
    "cats_df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "# 3) Use FP-Growth\n",
    "freq_itemsets_A3 = fpgrowth(cats_df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# 4) Generate rules\n",
    "rules_A3 = association_rules(freq_itemsets_A3, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "print(f\"‚úÖ Model A3: {len(rules_A3):,} rules found\")\n",
    "\n",
    "# Summary function\n",
    "def summarize_rules(rules, name):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Number of rules: {len(rules):,}\")\n",
    "    print(f\"Support range: {rules['support'].min():.4f} ‚Üí {rules['support'].max():.4f}\")\n",
    "    print(f\"Confidence range: {rules['confidence'].min():.4f} ‚Üí {rules['confidence'].max():.4f}\")\n",
    "    print(f\"Lift range: {rules['lift'].min():.4f} ‚Üí {rules['lift'].max():.4f}\")\n",
    "    \n",
    "    # Show top 5 rules by confidence\n",
    "    if len(rules) > 0:\n",
    "        print(\"\\nTop 5 rules by confidence:\")\n",
    "        top_rules = rules.sort_values('confidence', ascending=False).head(5)\n",
    "        for idx, row in top_rules.iterrows():\n",
    "            print(f\"  {set(row['antecedents'])} ‚Üí {set(row['consequents'])}\")\n",
    "            print(f\"    Support: {row['support']:.4f}, Confidence: {row['confidence']:.4f}, Lift: {row['lift']:.4f}\")\n",
    "\n",
    "# Summarize all models\n",
    "summarize_rules(rules_A1, \"A1: skill-level\")\n",
    "summarize_rules(rules_A2, \"A2: category-level\")\n",
    "summarize_rules(rules_A3, \"A3: combined\")\n",
    "\n",
    "# Save results\n",
    "print(\"\\nüíæ Saving association rules results...\")\n",
    "rules_A1.to_csv(\"/kaggle/working/association_rules_skills.csv\", index=False)\n",
    "rules_A2.to_csv(\"/kaggle/working/association_rules_categories.csv\", index=False)\n",
    "rules_A3.to_csv(\"/kaggle/working/association_rules_combined.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ All association rules saved to files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f262fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üíæ EXPORTING TRAINED MODELS FOR APPLICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Export Model A2 (Category-Level) - THIS IS WHAT THE APP USES\n",
    "print(\"\\nüì¶ Exporting Model A2 (Category-Level Rules)...\")\n",
    "print(\"   ‚îú‚îÄ This model is used for skill recommendations in the app\")\n",
    "print(\"   ‚îú‚îÄ It recommends related skills based on user's current skill categories\")\n",
    "print(\"   ‚îî‚îÄ Required for /app/models/association_rules.pkl\")\n",
    "\n",
    "association_model_a2 = {\n",
    "    'rules': rules_A2,                           # DataFrame with all association rules\n",
    "    'frequent_itemsets': freq_itemsets_A2,       # Frequent itemsets for reference\n",
    "    'model_type': 'A2_categories',               # Model identifier\n",
    "    'min_support': 0.01,                         # Parameters used\n",
    "    'min_confidence': 0.4,\n",
    "    'algorithm': 'apriori'\n",
    "}\n",
    "\n",
    "joblib.dump(association_model_a2, '/kaggle/working/association_rules_a2.pkl')\n",
    "print(\"‚úÖ Saved: /kaggle/working/association_rules_a2.pkl\")\n",
    "\n",
    "# Export Model A1 (Skill-Level) - FOR REFERENCE/FUTURE USE\n",
    "print(\"\\nüì¶ Exporting Model A1 (Skill-Level Rules)...\")\n",
    "association_model_a1 = {\n",
    "    'rules': rules_A1,\n",
    "    'frequent_itemsets': freq_itemsets_A1,\n",
    "    'model_type': 'A1_skills',\n",
    "    'min_support': 0.01,\n",
    "    'min_confidence': 0.4,\n",
    "    'algorithm': 'fpgrowth'\n",
    "}\n",
    "\n",
    "joblib.dump(association_model_a1, '/kaggle/working/association_rules_a1.pkl')\n",
    "print(\"‚úÖ Saved: /kaggle/working/association_rules_a1.pkl\")\n",
    "\n",
    "# Export Model A3 (Combined) - FOR REFERENCE/FUTURE USE\n",
    "print(\"\\nüì¶ Exporting Model A3 (Combined Rules)...\")\n",
    "association_model_a3 = {\n",
    "    'rules': rules_A3,\n",
    "    'frequent_itemsets': freq_itemsets_A3,\n",
    "    'model_type': 'A3_combined',\n",
    "    'min_support': 0.01,\n",
    "    'min_confidence': 0.4,\n",
    "    'algorithm': 'fpgrowth'\n",
    "}\n",
    "\n",
    "joblib.dump(association_model_a3, '/kaggle/working/association_rules_a3.pkl')\n",
    "print(\"‚úÖ Saved: /kaggle/working/association_rules_a3.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ MODEL EXPORT COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüìã Summary:\")\n",
    "print(f\"  ‚Ä¢ Model A2 Rules: {len(rules_A2):,} rules\")\n",
    "print(f\"  ‚Ä¢ Model A2 Quality: Avg Confidence={rules_A2['confidence'].mean():.1%}, Avg Lift={rules_A2['lift'].mean():.2f}\")\n",
    "print(\"\\nüí° Next Step:\")\n",
    "print(\"  1. Download association_rules_a2.pkl from Kaggle\")\n",
    "print(\"  2. Place in: app/models/association_rules.pkl\")\n",
    "print(\"  3. The app will use this for generating skill recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f33f4",
   "metadata": {},
   "source": [
    "## Using the Trained Association Models in the App\n",
    "\n",
    "This section demonstrates how the trained association rule models (A1, A2, A3) are deployed and used by the Streamlit application for real-time skill recommendations.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Unsupervised Learning**: Association rules are extracted using unsupervised methods (FP-Growth, Apriori).\n",
    "  No labeled training data is needed‚Äîonly transaction histories (job skill profiles).\n",
    "  \n",
    "- **Three Models with Different Perspectives**:\n",
    "  - **A1 (Skill-level, FP-Growth)**: Finds direct relationships between individual skills\n",
    "  - **A2 (Category-level, Apriori)**: Finds relationships between skill categories (broader patterns)\n",
    "  - **A3 (Combined, FP-Growth)**: Finds relationships mixing skills and categories\n",
    "  \n",
    "- **Best Model for Deployment**: **Model A2** is used as the primary recommendation model in the app because:\n",
    "  - Categories are more interpretable to users\n",
    "  - Reduces noise from low-frequency skills\n",
    "  - Provides stable, actionable recommendations\n",
    "  \n",
    "- **Ensemble Approach**: The app loads all three models (A1, A2, A3) via `AssociationEnsemble`, which:\n",
    "  - Queries each model independently\n",
    "  - Combines scores using \"noisy-or\" aggregation\n",
    "  - Returns ranked, deduplicated recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68849ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE: How the Streamlit App Uses Association Rules for Recommendations\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DEMONSTRATING: ASSOCIATION RULES FOR SKILL RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simulating a user's current skills\n",
    "user_skills_example = ['python', 'sql', 'git']\n",
    "target_job_skills = ['python', 'sql', 'spark', 'scala', 'hadoop', 'machine learning']\n",
    "\n",
    "print(f\"\\nüìù Example User Profile:\")\n",
    "print(f\"   Current Skills: {user_skills_example}\")\n",
    "print(f\"   Target Job Skills: {target_job_skills}\")\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 1: Using AssociationEnsemble (What the App Uses)\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"METHOD 1: Using AssociationEnsemble (Primary App Approach)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "from src.models.association_miner import AssociationEnsemble, get_skill_recommendations_with_explanations\n",
    "\n",
    "try:\n",
    "    # This is exactly what the Streamlit app does\n",
    "    ensemble = AssociationEnsemble()\n",
    "    \n",
    "    # Load all three models (A1: skills, A2: categories, A3: combined)\n",
    "    # Note: In the app, it uses get_association_rules_from_csv() which handles file loading\n",
    "    ensemble.load_paths([\n",
    "        'data/processed/association_rules_categories.csv',      # A2 (primary)\n",
    "        'data/processed/association_rules_skills.csv',          # A1 (secondary)\n",
    "        'data/processed/association_rules_combined.csv'         # A3 (tertiary)\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\n‚úÖ Loaded {len(ensemble.models)} association rule models\")\n",
    "    \n",
    "    # Get recommendations for the user's current skills\n",
    "    recommendations_df = ensemble.get_recommendations(user_skills_example, top_n=5)\n",
    "    \n",
    "    print(f\"\\nüìä Top 5 Recommended Skills (from ensemble):\")\n",
    "    print(recommendations_df[['skill', 'score', 'sources', 'top_source']].to_string(index=False))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Note: Full integration test requires CSV files. Error: {str(e)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 2: Using High-Level Helper Function with Explanations\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"METHOD 2: Using High-Level Helper with Human-Readable Explanations\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"(This is what you see in the Streamlit UI)\")\n",
    "\n",
    "try:\n",
    "    result = get_skill_recommendations_with_explanations(\n",
    "        user_skills=user_skills_example,\n",
    "        target_job_skills=target_job_skills,\n",
    "        data_dir='data/processed',\n",
    "        top_n=5\n",
    "    )\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\n‚úÖ Successfully generated {len(result['recommendations'])} recommendations\")\n",
    "        print(f\"   Using {result['num_rules_loaded']:,} association rules\\n\")\n",
    "        \n",
    "        for rec in result['recommendations']:\n",
    "            print(f\"üìö {rec['skill'].upper()}\")\n",
    "            print(f\"   Explanation: {rec['explanation']}\")\n",
    "            print(f\"   Score: {rec['score']:.1%}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è  {result['error_message']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error: {str(e)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPLANATION: Why These Recommendations?\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"UNDERSTANDING THE RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "explanation = \"\"\"\n",
    "The association rules work by finding patterns in job profiles. For example:\n",
    "\n",
    "1. **Confidence**: Of all jobs where users have [Python, SQL], \n",
    "   what percentage also have [Spark]?\n",
    "   \n",
    "2. **Lift**: Is [Spark] more likely to appear with [Python, SQL] \n",
    "   than by random chance?\n",
    "   \n",
    "3. **Support**: What percentage of all jobs have both [Python] AND [Spark]?\n",
    "\n",
    "When you have skills [Python, SQL, Git], the algorithm:\n",
    "1. Finds all rules where these skills appear in the antecedents\n",
    "2. Extracts the consequents (recommended skills)\n",
    "3. Ranks by confidence and lift\n",
    "4. Deduplicates across multiple models (A1, A2, A3)\n",
    "5. Returns the top N with explanations\n",
    "\n",
    "This is UNSUPERVISED learning because:\n",
    "- No labeled data (no \"this person should learn X\" examples)\n",
    "- Only learning from patterns in job skill co-occurrences\n",
    "- No ML model parameters to tune (just support/confidence thresholds)\n",
    "\"\"\"\n",
    "\n",
    "print(explanation)\n",
    "\n",
    "# ============================================================================\n",
    "# PRODUCTION SUMMARY\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"PRODUCTION DEPLOYMENT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = \"\"\"\n",
    "‚úÖ ASSIGNMENT REQUIREMENT: Association Rules for Skill Recommendations\n",
    "\n",
    "1. ‚úì Unsupervised Learning: FP-Growth (A1, A3) and Apriori (A2) algorithms\n",
    "2. ‚úì Multiple Datasets: all_jobs.csv + skill_migration.csv \n",
    "3. ‚úì Preprocessing: Data cleaning in notebooks 00-01, extraction in this notebook\n",
    "4. ‚úì Multiple Models: A1 (skills, FP-Growth), A2 (categories, Apriori), A3 (combined, FP-Growth)\n",
    "5. ‚úì Deployed Model: A2 (categories) in Streamlit app via AssociationEnsemble\n",
    "\n",
    "üéØ In the Streamlit App:\n",
    "   - User selects skills ‚Üí selects target job\n",
    "   - Gap analysis shows missing skills\n",
    "   - ‚≠ê NEW: \"AI-Powered Skill Recommendations\" section shows association rules\n",
    "   - Recommendations have explanations (e.g., \"Users with Python+SQL often learn Spark\")\n",
    "   - Learning path enriched with rule-based justifications\n",
    "\n",
    "üìä Model Performance:\n",
    "   - A1: {len(rules_A1):,} rules (skill-level patterns)\n",
    "   - A2: {len(rules_A2):,} rules (category-level patterns)\n",
    "   - A3: {len(rules_A3):,} rules (combined patterns)\n",
    "\n",
    "üíª Code Integration:\n",
    "   - app/main.py: New section displaying association rules recommendations\n",
    "   - src/models/association_miner.py: AssociationEnsemble.get_recommendations()\n",
    "   - src/models/learning_path_generator.py: enrich_learning_path_with_associations()\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8879404,
     "sourceId": 14015965,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1640.72169,
   "end_time": "2025-12-05T22:49:04.622943",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-05T22:21:43.901253",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
