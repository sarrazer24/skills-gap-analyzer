{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13934629,"sourceType":"datasetVersion","datasetId":8879404}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sarraverse/01-data-cleaning?scriptVersionId=284133537\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T18:21:34.61367Z","iopub.execute_input":"2025-12-05T18:21:34.614072Z","iopub.status.idle":"2025-12-05T18:21:36.550749Z","shell.execute_reply.started":"2025-12-05T18:21:34.614041Z","shell.execute_reply":"2025-12-05T18:21:36.549827Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/job-skills/skill_migration_public.csv\n/kaggle/input/job-skills/all_jobs_clustered_full.csv\n/kaggle/input/job-skills/all_jobs_mapped.csv\n/kaggle/input/job-skills/all_jobs_clean_full.csv\n/kaggle/input/job-skills/skill_migration_clean.csv\n/kaggle/input/job-skills/all_jobs.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# DATA CLEANING AND PREPROCESSING","metadata":{}},{"cell_type":"code","source":"\"\"\"\nDATA CLEANING AND PREPROCESSING\nTeacher Requirement: Apply preprocessing techniques according to the application domain\n\"\"\"\nimport pandas as pd\nimport re\nimport os\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\nprint(\"üìä DATA CLEANING PIPELINE\")\n\n# ====================\n# STEP 1: Clean all_jobs dataset\n# ====================\nprint(\"STEP 1: Cleaning job postings dataset\")\n\ninput_path = \"/kaggle/input/job-skills/all_jobs.csv\"\noutput_path = \"/kaggle/working/all_jobs_clean_full.csv\"\nchunksize = 50000\n\n# üîê for GLOBAL duplicate removal\nseen_job_keys = set()       # for drop_duplicates(subset=[\"job_key\"])\nseen_composite_keys = set() # for drop_duplicates subset combo\n\n# 1Ô∏è‚É£ Your cleaning function, vectorized-style\ndef clean_text_series(s: pd.Series):\n    return (\n        s.astype(str)\n        .str.replace(\"\\n\", \" \", regex=False)\n        .str.replace(\"\\r\", \" \", regex=False)\n        .str.replace(r\"\\s+\", \" \", regex=True)\n        .str.strip()\n    )\n\n# 2Ô∏è‚É£ same logic: empty row = no title, no desc, no skills\ndef drop_fully_empty_rows(df: pd.DataFrame) -> pd.DataFrame:\n    def is_empty(x):\n        return (pd.isna(x)) or (str(x).strip() == \"\")\n\n    mask_keep = ~(\n        df[\"job_title\"].apply(is_empty) &\n        df[\"job_description\"].apply(is_empty) &\n        df[\"skills_raw\"].apply(is_empty)\n    )\n    return df[mask_keep].copy()\n\n# 3Ô∏è‚É£ Create composite key for deduplication\ndef make_composite_key(row):\n    parts = [\n        str(row.get(\"job_title\", \"\")).lower(),\n        str(row.get(\"company\", \"\")).lower(),\n        str(row.get(\"location\", \"\")).lower(),\n        str(row.get(\"job_description\", \"\")).lower()[:200],\n    ]\n    return \"||\".join(parts)\n\n# 4Ô∏è‚É£ Process each chunk\ndef process_chunk(chunk: pd.DataFrame):\n    global seen_job_keys, seen_composite_keys\n\n    # Drop very sparse columns if they exist\n    for col in [\"responsibilities\", \"qualifications\", \"country\"]:\n        if col in chunk.columns:\n            chunk = chunk.drop(columns=[col])\n\n    # Fill missing values for text columns\n    for col in [\"job_title\", \"job_description\", \"skills_raw\"]:\n        if col in chunk.columns:\n            chunk[col] = chunk[col].fillna(\"\")\n\n    # Step 1: Drop rows with empty title+desc+skills\n    chunk = drop_fully_empty_rows(chunk)\n\n    # Step 2: Clean text\n    cols_to_clean = [\"job_title\", \"job_description\", \"skills_raw\",\n                     \"location\", \"company\", \"country\"]\n    for col in cols_to_clean:\n        if col in chunk.columns:\n            chunk[col] = clean_text_series(chunk[col])\n\n    # Step 3: Filter short title/desc\n    if \"job_title\" in chunk.columns:\n        chunk = chunk[chunk[\"job_title\"].str.len() >= 5]\n    if \"job_description\" in chunk.columns:\n        chunk = chunk[chunk[\"job_description\"].str.len() >= 30]\n\n    # Step 4: Global dedup by job_key\n    if \"job_key\" in chunk.columns:\n        mask_new_key = ~chunk[\"job_key\"].isin(seen_job_keys)\n        chunk = chunk[mask_new_key].copy()\n        seen_job_keys.update(chunk[\"job_key\"].tolist())\n\n    # Step 5: Global dedup by composite key\n    chunk[\"__dedup_key__\"] = chunk.apply(make_composite_key, axis=1)\n    mask_new_combo = ~chunk[\"__dedup_key__\"].isin(seen_composite_keys)\n    chunk = chunk[mask_new_combo].copy()\n    seen_composite_keys.update(chunk[\"__dedup_key__\"].tolist())\n    chunk = chunk.drop(columns=[\"__dedup_key__\"])\n\n    return chunk\n\n# 5Ô∏è‚É£ Main cleaning function\ndef run_full_clean():\n    print(f\"Reading from: {input_path}\")\n    print(f\"Writing to: {output_path}\")\n    \n    first = True\n    total_rows = 0\n    \n    # Get initial stats from original file\n    print(\"üìà Getting initial statistics...\")\n    original_sample = pd.read_csv(input_path, nrows=10000)\n    print(f\"Sample shape: {original_sample.shape}\")\n    \n    for chunk in pd.read_csv(input_path, chunksize=chunksize):\n        total_rows += len(chunk)\n        print(f\"Processing chunk {total_rows//chunksize + 1}...\")\n        \n        cleaned_chunk = process_chunk(chunk)\n        \n        # Save cleaned chunk\n        cleaned_chunk.to_csv(\n            output_path,\n            mode=\"w\" if first else \"a\",\n            header=first,\n            index=False\n        )\n        first = False\n    \n    print(f\"‚úÖ Finished processing {total_rows:,} rows\")\n    print(f\"Saved cleaned data to: {output_path}\")\n    \n    # Load cleaned data for visualization\n    print(\"üìä Loading cleaned data for visualization...\")\n    if os.path.exists(output_path):\n        cleaned_df = pd.read_csv(output_path, nrows=10000)\n        \n        # AFTER PREPROCESSING VISUALIZATIONS\n        print(\"\\n\" + \"=\"*50)\n        print(\"AFTER PREPROCESSING RESULTS\")\n        print(\"=\"*50)\n        \n        # Create comparison\n        comparison = pd.DataFrame({\n            'Metric': ['Rows', 'Columns', 'Memory (MB)', 'Missing Values %'],\n            'Before': [\n                len(original_sample),\n                original_sample.shape[1],\n                original_sample.memory_usage(deep=True).sum() / (1024**2),\n                original_sample.isna().mean().mean() * 100\n            ],\n            'After': [\n                len(cleaned_df),\n                cleaned_df.shape[1],\n                cleaned_df.memory_usage(deep=True).sum() / (1024**2),\n                cleaned_df.isna().mean().mean() * 100\n            ]\n        })\n        \n        print(\"\\nüìã Comparison Table:\")\n        print(comparison.to_string(index=False))\n        \n        # Create visualization\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n        \n        # 1. Missing values comparison\n        original_missing = original_sample.isna().mean().sort_values(ascending=False).head(5)\n        cleaned_missing = cleaned_df.isna().mean().sort_values(ascending=False).head(5)\n        \n        x = range(len(original_missing))\n        axes[0,0].bar(x, original_missing.values, alpha=0.6, width=0.4, label='Before', align='edge')\n        axes[0,0].bar([i + 0.4 for i in x], cleaned_missing.values, alpha=0.6, width=0.4, label='After', align='edge')\n        axes[0,0].set_xticks([i + 0.2 for i in x])\n        axes[0,0].set_xticklabels(original_missing.index, rotation=45, ha='right')\n        axes[0,0].set_ylabel('Missing Percentage')\n        axes[0,0].set_title('Missing Values Comparison (Top 5 Columns)')\n        axes[0,0].legend()\n        axes[0,0].grid(True, alpha=0.3)\n        \n        # 2. Text length distribution\n        axes[0,1].hist(original_sample['job_title'].str.len().dropna(), \n                      bins=30, alpha=0.6, label='Before', density=True)\n        axes[0,1].hist(cleaned_df['job_title'].str.len().dropna(), \n                      bins=30, alpha=0.6, label='After', density=True)\n        axes[0,1].set_xlabel('Title Length (characters)')\n        axes[0,1].set_ylabel('Density')\n        axes[0,1].set_title('Job Title Length Distribution')\n        axes[0,1].legend()\n        axes[0,1].grid(True, alpha=0.3)\n        \n        # 3. Skill count distribution\n        if 'skills_raw' in cleaned_df.columns:\n            original_skills = original_sample['skills_raw'].str.split(',').str.len().fillna(0)\n            cleaned_skills = cleaned_df['skills_raw'].str.split(',').str.len().fillna(0)\n            \n            axes[1,0].hist(original_skills, bins=20, alpha=0.6, label='Before', density=True)\n            axes[1,0].hist(cleaned_skills, bins=20, alpha=0.6, label='After', density=True)\n            axes[1,0].set_xlabel('Number of Skills')\n            axes[1,0].set_ylabel('Density')\n            axes[1,0].set_title('Skill Count Distribution')\n            axes[1,0].legend()\n            axes[1,0].grid(True, alpha=0.3)\n        \n        # 4. Summary statistics\n        axes[1,1].axis('off')\n        summary_text = f\"\"\"üìä CLEANING SUMMARY\n\nOriginal Sample: {len(original_sample):,} rows\nCleaned Sample: {len(cleaned_df):,} rows\nReduction: {(1 - len(cleaned_df)/len(original_sample))*100:.1f}%\n\nColumns Removed: {original_sample.shape[1] - cleaned_df.shape[1]}\nMemory Saved: {(original_sample.memory_usage(deep=True).sum() - cleaned_df.memory_usage(deep=True).sum())/(1024**2):.1f} MB\n\nData Quality Improved:\n‚úì Removed empty rows\n‚úì Cleaned text fields\n‚úì Deduplicated entries\n‚úì Filtered invalid entries\"\"\"\n        \n        axes[1,1].text(0.1, 0.5, summary_text, fontsize=10, va='center')\n        \n        plt.tight_layout()\n        plt.savefig('/kaggle/working/data_cleaning_results.png', dpi=150, bbox_inches='tight')\n        plt.show()\n        \n        return cleaned_df\n    else:\n        print(\"‚ùå Error: Cleaned file not created!\")\n        return None\n\n# ====================\n# STEP 2: Clean skill migration dataset\n# ====================\ndef clean_skills_taxonomy():\n    print(\"\\n\" + \"=\"*50)\n    print(\"STEP 2: Cleaning skills taxonomy dataset\")\n    print(\"=\"*50)\n    \n    # 1. Load raw file\n    skills_raw = pd.read_csv(\"/kaggle/input/job-skills/skill_migration_public.csv\")\n    print(f\"Raw skills shape: {skills_raw.shape}\")\n    \n    # 2. Keep only useful columns\n    skills = skills_raw[[\n        \"skill_group_name\",\n        \"skill_group_category\"\n    ]].copy()\n    \n    # 3. Drop rows where skill name is missing\n    skills = skills[skills[\"skill_group_name\"].notna()].copy()\n    print(f\"After dropping NA skills: {skills.shape}\")\n    \n    # 4. Normalize text (lowercase, strip)\n    for col in [\"skill_group_name\", \"skill_group_category\"]:\n        if col in skills.columns:\n            skills[col] = (\n                skills[col]\n                .astype(str)\n                .str.strip()\n                .str.lower()\n            )\n    \n    # 5. Drop exact duplicates\n    skills = skills.drop_duplicates(subset=[\"skill_group_name\", \"skill_group_category\"])\n    print(f\"After dropping duplicates: {skills.shape}\")\n    \n    # 6. If one skill appears with multiple categories, keep the first one\n    skills = skills.drop_duplicates(subset=[\"skill_group_name\"], keep=\"first\")\n    print(f\"After resolving skill conflicts: {skills.shape}\")\n    \n    # 7. Save cleaned skills\n    skills_output_path = \"/kaggle/working/skill_migration_clean.csv\"\n    skills.to_csv(skills_output_path, index=False)\n    print(f\"‚úÖ Saved cleaned skills to: {skills_output_path}\")\n    \n    # 8. Show sample\n    print(\"\\nüìã Sample of cleaned skills:\")\n    print(skills.head(10))\n    \n    # 9. Visualize skill categories\n    plt.figure(figsize=(10, 6))\n    category_counts = skills[\"skill_group_category\"].value_counts().head(15)\n    category_counts.plot(kind='bar')\n    plt.title('Top 15 Skill Categories')\n    plt.xlabel('Category')\n    plt.ylabel('Number of Skills')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.savefig('/kaggle/working/skill_categories_distribution.png', dpi=150)\n    plt.show()\n    \n    return skills\n\n# ====================\n# STEP 3: Map skills to jobs\n# ====================\ndef map_skills_to_jobs():\n    print(\"\\n\" + \"=\"*50)\n    print(\"STEP 3: Mapping skills to job postings\")\n    print(\"=\"*50)\n    \n    # Check if cleaned jobs file exists\n    jobs_file = \"/kaggle/working/all_jobs_clean_full.csv\"\n    if not os.path.exists(jobs_file):\n        print(\"‚ùå Error: Cleaned jobs file not found. Run Step 1 first.\")\n        return None\n    \n    # Load skills dictionary\n    skills_file = \"/kaggle/working/skill_migration_clean.csv\"\n    if not os.path.exists(skills_file):\n        print(\"‚ùå Error: Cleaned skills file not found. Run Step 2 first.\")\n        return None\n    \n    skills_meta = pd.read_csv(skills_file)\n    \n    # Create skill dictionary\n    skill_dict = (\n        skills_meta[[\"skill_group_name\", \"skill_group_category\"]]\n        .drop_duplicates()\n        .set_index(\"skill_group_name\")[\"skill_group_category\"]\n        .to_dict()\n    )\n    \n    print(f\"Loaded {len(skill_dict)} skills for mapping.\")\n    \n    # Helper functions\n    def parse_skills(raw):\n        if pd.isna(raw):\n            return []\n        text = str(raw).strip().lower()\n        if not text:\n            return []\n        # Split by common separators\n        parts = re.split(r\",|/|;|\\||\\+\", text)\n        return [p.strip() for p in parts if p.strip()]\n    \n    def map_skill_categories(skill_list):\n        cats = set()\n        for s in skill_list:\n            cat = skill_dict.get(s)\n            if cat is None:\n                cat = \"other\"\n            cats.add(cat)\n        if not cats:\n            return \"\"\n        return \",\".join(sorted(cats))\n    \n    # Process in chunks\n    input_path = \"/kaggle/working/all_jobs_clean_full.csv\"\n    output_path = \"/kaggle/working/all_jobs_mapped.csv\"\n    chunksize = 50000\n    \n    print(f\"Reading from: {input_path}\")\n    print(f\"Writing to: {output_path}\")\n    \n    first = True\n    total_rows = 0\n    \n    for chunk in pd.read_csv(input_path, chunksize=chunksize):\n        total_rows += len(chunk)\n        print(f\"Processing chunk {total_rows//chunksize + 1} ({len(chunk)} rows)...\")\n        \n        # Ensure skills_raw column exists\n        if \"skills_raw\" not in chunk.columns:\n            chunk[\"skills_raw\"] = \"\"\n        \n        # Parse skills\n        chunk[\"skill_list\"] = chunk[\"skills_raw\"].apply(parse_skills)\n        \n        # Map to categories\n        chunk[\"skill_categories\"] = chunk[\"skill_list\"].apply(map_skill_categories)\n        \n        # Save mapped chunk\n        chunk.to_csv(\n            output_path,\n            mode=\"w\" if first else \"a\",\n            header=first,\n            index=False\n        )\n        first = False\n    \n    print(f\"‚úÖ Mapped {total_rows:,} jobs to skills\")\n    \n    # Load and show sample of mapped data\n    mapped_sample = pd.read_csv(output_path, nrows=1000)\n    print(f\"\\nüìã Sample of mapped data:\")\n    print(f\"Shape: {mapped_sample.shape}\")\n    \n    # Show skill statistics\n    all_skills = [skill for sublist in mapped_sample[\"skill_list\"].apply(eval) for skill in sublist]\n    print(f\"Total skill instances: {len(all_skills)}\")\n    print(f\"Unique skills found: {len(set(all_skills))}\")\n    \n    # Visualize skill frequency\n    if len(all_skills) > 0:\n        from collections import Counter\n        skill_counts = Counter(all_skills)\n        top_skills = skill_counts.most_common(20)\n        \n        plt.figure(figsize=(12, 6))\n        skills, counts = zip(*top_skills)\n        plt.barh(range(len(skills)), counts)\n        plt.yticks(range(len(skills)), skills)\n        plt.xlabel('Frequency')\n        plt.title('Top 20 Most Frequent Skills')\n        plt.gca().invert_yaxis()\n        plt.tight_layout()\n        plt.savefig('/kaggle/working/top_skills_distribution.png', dpi=150)\n        plt.show()\n    \n    return mapped_sample\n\n# ====================\n# MAIN EXECUTION\n# ====================\nif __name__ == \"__main__\":\n    print(\"üöÄ STARTING DATA CLEANING PIPELINE\")\n    print(\"=\"*60)\n    \n    # Step 1: Clean jobs data\n    print(\"\\nüìÅ STEP 1: Cleaning job postings...\")\n    cleaned_jobs = run_full_clean()\n    \n    # Step 2: Clean skills taxonomy\n    print(\"\\nüìÅ STEP 2: Cleaning skills taxonomy...\")\n    cleaned_skills = clean_skills_taxonomy()\n    \n    # Step 3: Map skills to jobs\n    print(\"\\nüìÅ STEP 3: Mapping skills to jobs...\")\n    if cleaned_jobs is not None and cleaned_skills is not None:\n        mapped_data = map_skills_to_jobs()\n        \n        # Final summary\n        print(\"\\n\" + \"=\"*60)\n        print(\"‚úÖ DATA CLEANING PIPELINE COMPLETE!\")\n        print(\"=\"*60)\n        \n        # List output files\n        output_files = [\n            \"/kaggle/working/all_jobs_clean_full.csv\",\n            \"/kaggle/working/skill_migration_clean.csv\", \n            \"/kaggle/working/all_jobs_mapped.csv\",\n            \"/kaggle/working/data_cleaning_results.png\",\n            \"/kaggle/working/skill_categories_distribution.png\",\n            \"/kaggle/working/top_skills_distribution.png\"\n        ]\n        \n        print(\"\\nüìÅ OUTPUT FILES CREATED:\")\n        for file in output_files:\n            if os.path.exists(file):\n                size_mb = os.path.getsize(file) / (1024**2)\n                print(f\"  ‚úì {os.path.basename(file)} ({size_mb:.1f} MB)\")\n            else:\n                print(f\"  ‚úó {os.path.basename(file)} (not found)\")\n    else:\n        print(\"\\n‚ùå Pipeline failed. Check error messages above.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T18:21:36.55269Z","iopub.execute_input":"2025-12-05T18:21:36.553046Z","iopub.status.idle":"2025-12-05T18:36:12.37822Z","shell.execute_reply.started":"2025-12-05T18:21:36.553026Z","shell.execute_reply":"2025-12-05T18:36:12.376969Z"}},"outputs":[{"name":"stdout","text":"üìä DATA CLEANING PIPELINE\nSTEP 1: Cleaning job postings dataset\nüöÄ STARTING DATA CLEANING PIPELINE\n============================================================\n\nüìÅ STEP 1: Cleaning job postings...\nReading from: /kaggle/input/job-skills/all_jobs.csv\nWriting to: /kaggle/working/all_jobs_clean_full.csv\nüìà Getting initial statistics...\nSample shape: (10000, 10)\nProcessing chunk 2...\nProcessing chunk 3...\nProcessing chunk 4...\nProcessing chunk 5...\nProcessing chunk 6...\nProcessing chunk 7...\nProcessing chunk 8...\nProcessing chunk 9...\nProcessing chunk 10...\nProcessing chunk 11...\nProcessing chunk 12...\nProcessing chunk 13...\nProcessing chunk 14...\nProcessing chunk 15...\nProcessing chunk 16...\nProcessing chunk 17...\nProcessing chunk 18...\nProcessing chunk 19...\nProcessing chunk 20...\nProcessing chunk 21...\nProcessing chunk 22...\nProcessing chunk 23...\nProcessing chunk 24...\nProcessing chunk 25...\nProcessing chunk 26...\nProcessing chunk 27...\nProcessing chunk 28...\nProcessing chunk 29...\nProcessing chunk 30...\nProcessing chunk 31...\nProcessing chunk 32...\nProcessing chunk 33...\nProcessing chunk 34...\nProcessing chunk 35...\nProcessing chunk 36...\nProcessing chunk 37...\nProcessing chunk 38...\nProcessing chunk 39...\nProcessing chunk 40...\nProcessing chunk 41...\nProcessing chunk 42...\nProcessing chunk 43...\nProcessing chunk 44...\nProcessing chunk 45...\nProcessing chunk 46...\nProcessing chunk 47...\nProcessing chunk 48...\nProcessing chunk 49...\nProcessing chunk 50...\nProcessing chunk 51...\nProcessing chunk 52...\nProcessing chunk 53...\nProcessing chunk 54...\nProcessing chunk 55...\nProcessing chunk 56...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2359950758.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;31m# Step 1: Clean jobs data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüìÅ STEP 1: Cleaning job postings...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m     \u001b[0mcleaned_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_full_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;31m# Step 2: Clean skills taxonomy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/2359950758.py\u001b[0m in \u001b[0;36mrun_full_clean\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# Save cleaned chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         cleaned_chunk.to_csv(\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"w\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3965\u001b[0m         )\n\u001b[1;32m   3966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3967\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3968\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         )\n\u001b[0;32m-> 1014\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m             )\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         libwriters.write_csv_rows(\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mwriters.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":2}]}