{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e1138c",
   "metadata": {},
   "source": [
    "# Improved K-Means Clustering: Skill-Based Feature Engineering\n",
    "## Objective: Generate high-quality job similarity mapping for the Streamlit app\n",
    "\n",
    "This notebook improves clustering by:\n",
    "1. Using **skill-based features** instead of noisy free-text\n",
    "2. Testing multiple K values (20â€“80) with evaluation metrics\n",
    "3. Manually inspecting clusters for semantic coherence\n",
    "4. Implementing post-filtering logic for better \"similar opportunities\"\n",
    "5. Creating a compact mapping file for the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# ML and clustering\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ“š IMPROVED K-MEANS CLUSTERING FOR SKILL-BASED JOB SIMILARITY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nâœ… All libraries loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844227b2",
   "metadata": {},
   "source": [
    "## Section 1: Load and Explore Skill-Based Features\n",
    "\n",
    "Load the processed jobs file and verify skill_list availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = \"../data/processed/all_jobs_mapped.csv\"\n",
    "print(f\"Loading from: {data_path}\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"\\nðŸ“Š Dataset shape: {df.shape}\")\n",
    "print(f\"ðŸ“‹ Columns: {list(df.columns)}\")\n",
    "\n",
    "# Check for required columns\n",
    "required_cols = ['job_id', 'job_title', 'skill_list', 'location', 'company', 'skill_categories']\n",
    "for col in required_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"  âœ… {col}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {col} - MISSING\")\n",
    "\n",
    "# Parse skill_list column\n",
    "def parse_skill_list(x):\n",
    "    \"\"\"Convert skill_list from string representation to actual list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(str(x))\n",
    "        if isinstance(val, list):\n",
    "            return [str(s).strip().lower() for s in val]\n",
    "    except:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "df['skill_list'] = df['skill_list'].apply(parse_skill_list)\n",
    "\n",
    "# Display stats\n",
    "print(f\"\\nðŸ“ˆ Skill Statistics:\")\n",
    "print(f\"  Total jobs: {len(df)}\")\n",
    "print(f\"  Jobs with skills: {df['skill_list'].apply(len).gt(0).sum()}\")\n",
    "print(f\"  Avg skills per job: {df['skill_list'].apply(len).mean():.1f}\")\n",
    "print(f\"  Max skills in a job: {df['skill_list'].apply(len).max()}\")\n",
    "\n",
    "# Show samples\n",
    "print(f\"\\nðŸŽ¯ Sample Jobs:\")\n",
    "for idx in df.sample(min(3, len(df)), random_state=42).index:\n",
    "    job = df.iloc[idx]\n",
    "    print(f\"\\n  Job: {job['job_title']}\")\n",
    "    print(f\"  Location: {job['location']}\")\n",
    "    print(f\"  Skills ({len(job['skill_list'])}): {job['skill_list'][:5]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57ca97",
   "metadata": {},
   "source": [
    "## Section 2: Build Sparse Skill Vector Representation\n",
    "\n",
    "Create a binary skill matrix (1 if skill present, 0 otherwise) using only the most frequent skills to reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build skill vocabulary: only include skills appearing in >= min_freq jobs\n",
    "min_freq = 30  # Adjust based on data size\n",
    "all_skills = Counter()\n",
    "\n",
    "for skills in df['skill_list']:\n",
    "    all_skills.update(skills)\n",
    "\n",
    "# Keep only frequent skills\n",
    "skill_vocab = sorted([skill for skill, count in all_skills.most_common() if count >= min_freq])\n",
    "print(f\"ðŸ“Š Skill Vocabulary:\")\n",
    "print(f\"  Total unique skills: {len(all_skills)}\")\n",
    "print(f\"  Frequent skills (>= {min_freq} occurrences): {len(skill_vocab)}\")\n",
    "print(f\"  Top 20 skills by frequency:\")\n",
    "for skill, count in all_skills.most_common(20):\n",
    "    print(f\"    {skill:40} {count:5} jobs\")\n",
    "\n",
    "# Build binary skill matrix: rows=jobs, cols=skills\n",
    "def build_skill_matrix(df, skill_vocab):\n",
    "    \"\"\"Build sparse binary matrix where X[i,j] = 1 if job i has skill j.\"\"\"\n",
    "    n_jobs = len(df)\n",
    "    n_skills = len(skill_vocab)\n",
    "    skill_to_idx = {skill: idx for idx, skill in enumerate(skill_vocab)}\n",
    "    \n",
    "    print(f\"\\nðŸ”¨ Building sparse skill matrix ({n_jobs} jobs Ã— {n_skills} skills)...\")\n",
    "    \n",
    "    rows, cols, data = [], [], []\n",
    "    \n",
    "    for job_idx, skills in enumerate(df['skill_list']):\n",
    "        for skill in skills:\n",
    "            if skill in skill_to_idx:\n",
    "                rows.append(job_idx)\n",
    "                cols.append(skill_to_idx[skill])\n",
    "                data.append(1)\n",
    "    \n",
    "    X_sparse = csr_matrix((data, (rows, cols)), shape=(n_jobs, n_skills), dtype=np.int8)\n",
    "    \n",
    "    print(f\"âœ… Sparse matrix created: shape {X_sparse.shape}, density {X_sparse.nnz / (n_jobs * n_skills) * 100:.2f}%\")\n",
    "    return X_sparse, skill_vocab\n",
    "\n",
    "X_sparse, skill_vocab = build_skill_matrix(df, skill_vocab)\n",
    "\n",
    "# Keep a copy of the original dataframe for later reference\n",
    "df_original = df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5687da9",
   "metadata": {},
   "source": [
    "## Section 3: Tune K-Means with Multiple K Values\n",
    "\n",
    "Test K values from 20 to 80 and compute inertia + silhouette scores on a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c03067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune K-Means for multiple K values\n",
    "k_values = [20, 40, 60, 80]\n",
    "tuning_results = []\n",
    "\n",
    "# Use a sample for silhouette scoring (large datasets can be slow)\n",
    "sample_size = min(5000, len(df))\n",
    "sample_indices = np.random.choice(len(df), sample_size, replace=False)\n",
    "X_sample = X_sparse[sample_indices]\n",
    "\n",
    "print(f\"\\nðŸŽ¯ K-MEANS TUNING\")\n",
    "print(f\"Testing K values: {k_values}\")\n",
    "print(f\"Using sample of {sample_size} jobs for silhouette score evaluation\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"Training K-Means with K={k}...\")\n",
    "    \n",
    "    # Train on full dataset\n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters=k,\n",
    "        batch_size=1000,\n",
    "        random_state=42,\n",
    "        n_init=3,\n",
    "        max_iter=100\n",
    "    )\n",
    "    labels_full = kmeans.fit_predict(X_sparse)\n",
    "    \n",
    "    # Compute metrics\n",
    "    inertia = kmeans.inertia_\n",
    "    silhouette = silhouette_score(X_sample, kmeans.predict(X_sample), sample_size=1000)\n",
    "    \n",
    "    # Store results\n",
    "    tuning_results.append({\n",
    "        'K': k,\n",
    "        'Inertia': inertia,\n",
    "        'Silhouette': silhouette,\n",
    "        'Model': kmeans,\n",
    "        'Labels': labels_full\n",
    "    })\n",
    "    \n",
    "    print(f\"  âœ… K={k:2d} | Inertia: {inertia:12.0f} | Silhouette: {silhouette:.4f}\\n\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"â±ï¸  Tuning completed in {elapsed/60:.1f} minutes\")\n",
    "\n",
    "# Convert to DataFrame for easy comparison\n",
    "tuning_df = pd.DataFrame(tuning_results)\n",
    "print(\"\\nðŸ“Š K-Means Tuning Summary:\")\n",
    "print(tuning_df[['K', 'Inertia', 'Silhouette']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df28b8e",
   "metadata": {},
   "source": [
    "## Section 4: Evaluate Clusters with Inertia and Silhouette Score\n",
    "\n",
    "Visualize and select the optimal K based on metrics and manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2523354",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ˆ K-MEANS EVALUATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Normalize metrics for comparison\n",
    "tuning_df['Inertia_Norm'] = (tuning_df['Inertia'] - tuning_df['Inertia'].min()) / (tuning_df['Inertia'].max() - tuning_df['Inertia'].min())\n",
    "tuning_df['Silhouette_Norm'] = (tuning_df['Silhouette'] - tuning_df['Silhouette'].min()) / (tuning_df['Silhouette'].max() - tuning_df['Silhouette'].min())\n",
    "\n",
    "print(\"\\nMetrics Comparison (normalized 0-1):\")\n",
    "print(tuning_df[['K', 'Inertia_Norm', 'Silhouette_Norm']].to_string(index=False))\n",
    "\n",
    "# Find best K by each metric\n",
    "best_by_silhouette = tuning_df.loc[tuning_df['Silhouette'].idxmax(), 'K']\n",
    "best_by_inertia = tuning_df.loc[tuning_df['Inertia'].idxmin(), 'K']\n",
    "\n",
    "print(f\"\\nðŸ† Best by Silhouette Score: K={int(best_by_silhouette)}\")\n",
    "print(f\"ðŸ† Best by Inertia (Elbow): K={int(best_by_inertia)}\")\n",
    "print(f\"\\nðŸ’¡ Recommendation: Check cluster quality manually and select K based on semantic coherence.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf08b24",
   "metadata": {},
   "source": [
    "## Section 5: Inspect Cluster Quality by Job Titles and Skills\n",
    "\n",
    "For each K value, inspect representative clusters to assess semantic coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b93bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_clusters(df, labels, skill_vocab, X_sparse, k, num_samples=3):\n",
    "    \"\"\"Inspect cluster quality for a given K value.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CLUSTER INSPECTION: K={k}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df_temp = df_original.copy()\n",
    "    df_temp['cluster'] = labels\n",
    "    \n",
    "    # Get cluster sizes\n",
    "    cluster_sizes = df_temp['cluster'].value_counts().sort_index()\n",
    "    print(f\"\\nðŸ“Š Cluster Size Distribution:\")\n",
    "    print(f\"  Min: {cluster_sizes.min():5d} jobs\")\n",
    "    print(f\"  Max: {cluster_sizes.max():5d} jobs\")\n",
    "    print(f\"  Mean: {cluster_sizes.mean():5.0f} jobs\")\n",
    "    print(f\"  Std: {cluster_sizes.std():5.0f}\")\n",
    "    \n",
    "    # Sample clusters to inspect\n",
    "    sampled_clusters = np.random.choice(k, num_samples, replace=False)\n",
    "    \n",
    "    for cluster_id in sorted(sampled_clusters):\n",
    "        cluster_jobs = df_temp[df_temp['cluster'] == cluster_id]\n",
    "        cluster_mask = (df_temp['cluster'] == cluster_id).values\n",
    "        X_cluster = X_sparse[cluster_mask]\n",
    "        \n",
    "        print(f\"\\nðŸ” Cluster {cluster_id} ({len(cluster_jobs)} jobs)\")\n",
    "        print(f\"  {'â”€'*76}\")\n",
    "        \n",
    "        # Top job titles\n",
    "        print(f\"  Top 10 Job Titles:\")\n",
    "        top_jobs = cluster_jobs['job_title'].value_counts().head(10)\n",
    "        for job, count in top_jobs.items():\n",
    "            print(f\"    â€¢ {job:50} ({count:3d} occurrences)\")\n",
    "        \n",
    "        # Top skills in this cluster\n",
    "        print(f\"\\n  Top 20 Most Frequent Skills:\")\n",
    "        cluster_skills = Counter()\n",
    "        for skills in cluster_jobs['skill_list']:\n",
    "            cluster_skills.update(skills)\n",
    "        \n",
    "        for skill, count in cluster_skills.most_common(20):\n",
    "            pct = (count / len(cluster_jobs)) * 100\n",
    "            print(f\"    â€¢ {skill:40} {count:4d} jobs ({pct:5.1f}%)\")\n",
    "        \n",
    "        # Dominant skill categories (if available)\n",
    "        if 'skill_categories' in df_temp.columns:\n",
    "            print(f\"\\n  Skill Categories:\")\n",
    "            cats = cluster_jobs['skill_categories'].fillna('').value_counts().head(5)\n",
    "            for cat, count in cats.items():\n",
    "                if cat:\n",
    "                    print(f\"    â€¢ {cat}: {count} jobs\")\n",
    "\n",
    "# Inspect clusters for a few K values to assess quality\n",
    "for result in tuning_results[:2]:  # Inspect K=20 and K=40 for now\n",
    "    k = result['K']\n",
    "    labels = result['Labels']\n",
    "    inspect_clusters(df_original, labels, skill_vocab, X_sparse, k, num_samples=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12856011",
   "metadata": {},
   "source": [
    "## Section 6: Select Optimal K and Re-fit Model\n",
    "\n",
    "Based on silhouette scores, inertia, and manual inspection, select the best K and train final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b388d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT OPTIMAL K\n",
    "# Based on silhouette score and manual inspection, select the best K\n",
    "# You can adjust this based on your inspection results above\n",
    "\n",
    "optimal_k = 40  # Adjust based on silhouette score and cluster quality inspection\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ… SELECTED OPTIMAL K = {optimal_k}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get the model with optimal K\n",
    "optimal_result = next(r for r in tuning_results if r['K'] == optimal_k)\n",
    "optimal_kmeans = optimal_result['Model']\n",
    "optimal_labels = optimal_result['Labels']\n",
    "\n",
    "# Verify cluster distribution\n",
    "unique, counts = np.unique(optimal_labels, return_counts=True)\n",
    "print(f\"\\nðŸ“Š Final Cluster Distribution (K={optimal_k}):\")\n",
    "print(f\"  Clusters: {len(unique)}\")\n",
    "print(f\"  Min cluster size: {counts.min()}\")\n",
    "print(f\"  Max cluster size: {counts.max()}\")\n",
    "print(f\"  Mean cluster size: {counts.mean():.0f}\")\n",
    "print(f\"  Std cluster size: {counts.std():.0f}\")\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "df_original['cluster_id'] = optimal_labels\n",
    "\n",
    "print(f\"\\nâœ… Cluster assignments added to dataframe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b838008e",
   "metadata": {},
   "source": [
    "## Section 7: Create Compact Cluster Mapping\n",
    "\n",
    "Create a lightweight mapping file with job_id, title, company, location, cluster_id, and main_category for the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c28262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract dominant skill category per job\n",
    "def get_main_category(skill_categories_str):\n",
    "    \"\"\"Extract primary skill category from the concatenated string.\"\"\"\n",
    "    if pd.isna(skill_categories_str) or not skill_categories_str:\n",
    "        return 'other'\n",
    "    categories = str(skill_categories_str).split(',')\n",
    "    return categories[0].strip() if categories else 'other'\n",
    "\n",
    "# Build compact mapping\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“¦ CREATING COMPACT CLUSTER MAPPING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cluster_mapping = df_original[[\n",
    "    'job_id', 'job_title', 'company', 'location', 'cluster_id'\n",
    "]].copy()\n",
    "\n",
    "cluster_mapping['main_category'] = df_original['skill_categories'].apply(get_main_category)\n",
    "\n",
    "print(f\"\\nðŸ“Š Compact Mapping Shape: {cluster_mapping.shape}\")\n",
    "print(f\"ðŸ“‹ Columns: {list(cluster_mapping.columns)}\")\n",
    "print(f\"\\nðŸŽ¯ Sample Records:\")\n",
    "print(cluster_mapping.head(10).to_string(index=False))\n",
    "\n",
    "# Save as pickle (more efficient than CSV for repeated loading)\n",
    "output_path = \"../data/processed/job_clusters_small_v2.pkl\"\n",
    "cluster_mapping.to_pickle(output_path)\n",
    "print(f\"\\nâœ… Saved: {output_path}\")\n",
    "\n",
    "# Also save as CSV for debugging/inspection\n",
    "csv_path = output_path.replace('.pkl', '.csv')\n",
    "cluster_mapping.to_csv(csv_path, index=False)\n",
    "print(f\"âœ… Saved: {csv_path}\")\n",
    "\n",
    "# File size comparison\n",
    "import os\n",
    "pkl_size = os.path.getsize(output_path) / 1024 / 1024\n",
    "csv_size = os.path.getsize(csv_path) / 1024 / 1024\n",
    "print(f\"\\nðŸ’¾ File Sizes:\")\n",
    "print(f\"  Pickle: {pkl_size:.2f} MB\")\n",
    "print(f\"  CSV: {csv_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d409d184",
   "metadata": {},
   "source": [
    "## Section 8: Implement Post-Filter Logic for Similar Jobs\n",
    "\n",
    "Define helper functions to extract dominant skills and filter similar jobs by skill overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c67de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ”§ SKILL-BASED FILTERING LOGIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract dominant skills per job for filtering\n",
    "def get_top_skills(skill_list, n_top=5):\n",
    "    \"\"\"Get top N most common skills.\"\"\"\n",
    "    return skill_list[:n_top] if skill_list else []\n",
    "\n",
    "# Create a skills lookup table for filtering\n",
    "skills_lookup = pd.DataFrame({\n",
    "    'job_id': df_original['job_id'],\n",
    "    'top_skills': df_original['skill_list'].apply(lambda x: get_top_skills(x, n_top=5)),\n",
    "    'skill_categories': df_original['skill_categories'],\n",
    "    'location': df_original['location'],\n",
    "    'cluster_id': df_original['cluster_id']\n",
    "})\n",
    "\n",
    "def get_skill_overlap(skills_a, skills_b):\n",
    "    \"\"\"Compute skill overlap between two job skill lists.\"\"\"\n",
    "    set_a = set(skills_a)\n",
    "    set_b = set(skills_b)\n",
    "    overlap = set_a & set_b\n",
    "    return len(overlap), overlap\n",
    "\n",
    "def filter_similar_jobs(target_job_id, cluster_jobs_df, df_with_skills, min_skill_overlap=1):\n",
    "    \"\"\"\n",
    "    Filter similar jobs based on:\n",
    "    1. Same cluster\n",
    "    2. At least min_skill_overlap skills in common\n",
    "    3. Optional: same skill category\n",
    "    \n",
    "    Returns: filtered DataFrame and match_quality flag\n",
    "    \"\"\"\n",
    "    # Get target job skills\n",
    "    target_job = df_with_skills[df_with_skills['job_id'] == str(target_job_id)]\n",
    "    if target_job.empty:\n",
    "        return pd.DataFrame(), 'not_found'\n",
    "    \n",
    "    target_skills = target_job.iloc[0]['top_skills']\n",
    "    target_category = target_job.iloc[0]['skill_categories']\n",
    "    \n",
    "    # Filter candidates by skill overlap\n",
    "    filtered = []\n",
    "    for _, candidate in cluster_jobs_df.iterrows():\n",
    "        if candidate['job_id'] == str(target_job_id):\n",
    "            continue  # Skip the target job itself\n",
    "        \n",
    "        candidate_skills = candidate['top_skills']\n",
    "        overlap, _ = get_skill_overlap(target_skills, candidate_skills)\n",
    "        \n",
    "        if overlap >= min_skill_overlap:\n",
    "            filtered.append(candidate)\n",
    "    \n",
    "    if filtered:\n",
    "        return pd.DataFrame(filtered), 'high_quality'\n",
    "    else:\n",
    "        # Fallback: return unfiltered cluster jobs (loose match)\n",
    "        return cluster_jobs_df[cluster_jobs_df['job_id'] != str(target_job_id)], 'loose_match'\n",
    "\n",
    "print(\"\\nâœ… Filtering functions created\")\n",
    "print(\"\\nðŸ’¡ Filtering Strategy:\")\n",
    "print(\"   1. Find jobs in same cluster\")\n",
    "print(\"   2. Filter by >= 1 overlapping key skill\")\n",
    "print(\"   3. Fallback to unfiltered cluster if needed (marked as 'loose_match')\")\n",
    "\n",
    "# Test filtering with a sample nursing job\n",
    "sample_nursing = df_original[df_original['job_title'].str.contains('nurse', case=False, na=False)]\n",
    "if not sample_nursing.empty:\n",
    "    test_job_id = sample_nursing.iloc[0]['job_id']\n",
    "    test_job_title = sample_nursing.iloc[0]['job_title']\n",
    "    test_cluster = sample_nursing.iloc[0]['cluster_id']\n",
    "    \n",
    "    print(f\"\\nðŸ§ª TEST: Filtering similar jobs for '{test_job_title}'\")\n",
    "    print(f\"   Job ID: {test_job_id}\")\n",
    "    print(f\"   Cluster: {test_cluster}\")\n",
    "    \n",
    "    # Get cluster jobs\n",
    "    cluster_jobs = skills_lookup[skills_lookup['cluster_id'] == test_cluster]\n",
    "    \n",
    "    # Apply filtering\n",
    "    filtered_jobs, match_quality = filter_similar_jobs(\n",
    "        test_job_id,\n",
    "        cluster_jobs,\n",
    "        skills_lookup,\n",
    "        min_skill_overlap=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n   Original cluster size: {len(cluster_jobs)}\")\n",
    "    print(f\"   After filtering: {len(filtered_jobs)}\")\n",
    "    print(f\"   Match quality: {match_quality}\")\n",
    "    \n",
    "    if not filtered_jobs.empty:\n",
    "        print(f\"\\n   Filtered sample jobs:\")\n",
    "        for idx, job in filtered_jobs.head(3).iterrows():\n",
    "            print(f\"     â€¢ {cluster_mapping[cluster_mapping['job_id'] == job['job_id']]['job_title'].values[0] if not cluster_mapping[cluster_mapping['job_id'] == job['job_id']].empty else 'Unknown'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52cbb4",
   "metadata": {},
   "source": [
    "## Section 9: Export Results and Save for App Integration\n",
    "\n",
    "Save the fitted K-Means model, cluster mapping, and skills lookup for use in the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ffa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ’¾ SAVING RESULTS FOR STREAMLIT APP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Save the fitted K-Means model\n",
    "kmeans_model_path = \"../data/processed/kmeans_model_v2.pkl\"\n",
    "joblib.dump({\n",
    "    'model': optimal_kmeans,\n",
    "    'n_clusters': optimal_k,\n",
    "    'skill_vocab': skill_vocab,\n",
    "    'skill_to_idx': {skill: idx for idx, skill in enumerate(skill_vocab)},\n",
    "    'min_freq': min_freq,\n",
    "}, kmeans_model_path)\n",
    "print(f\"âœ… K-Means model: {kmeans_model_path}\")\n",
    "\n",
    "# 2. Save the cluster mapping (already saved earlier)\n",
    "print(f\"âœ… Cluster mapping: ../data/processed/job_clusters_small_v2.pkl\")\n",
    "\n",
    "# 3. Save skills lookup for filtering\n",
    "skills_lookup_path = \"../data/processed/skills_lookup_v2.pkl\"\n",
    "skills_lookup.to_pickle(skills_lookup_path)\n",
    "print(f\"âœ… Skills lookup: {skills_lookup_path}\")\n",
    "\n",
    "# 4. Save filtering functions as a module reference\n",
    "print(f\"\\nâœ… Filtering logic ready for src/models/cluster_analyzer.py\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"ðŸ“Š FINAL SUMMARY\")\n",
    "print(f\"=\"*80)\n",
    "print(f\"  Total jobs clustered: {len(cluster_mapping)}\")\n",
    "print(f\"  Number of clusters (K): {optimal_k}\")\n",
    "print(f\"  Avg jobs per cluster: {len(cluster_mapping) / optimal_k:.0f}\")\n",
    "print(f\"  Skill vocabulary size: {len(skill_vocab)}\")\n",
    "print(f\"  Feature matrix: {X_sparse.shape}\")\n",
    "print(f\"\\n  Saved Files:\")\n",
    "print(f\"    â€¢ job_clusters_small_v2.pkl (cluster mapping)\")\n",
    "print(f\"    â€¢ job_clusters_small_v2.csv (for inspection)\")\n",
    "print(f\"    â€¢ kmeans_model_v2.pkl (fitted model)\")\n",
    "print(f\"    â€¢ skills_lookup_v2.pkl (for filtering)\")\n",
    "print(f\"\\nâœ… CLUSTERING PIPELINE COMPLETE!\")\n",
    "print(f\"   Next: Update src/models/cluster_analyzer.py to load v2 files\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
